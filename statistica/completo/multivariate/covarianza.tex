\section{Covarianza}
La covarianza entra in gioco quando si ha a che fare con variabili aleatorie doppie o, più in
generale, variabili aleatorie multiple.

\begin{definition}
	Date $X$ e $Y$ due variabili aleatorie con momento secondo, si chiama \textbf{covarianza} tra
	$X$ e $Y$ la quantità
	\begin{align*}
		\Cov(X, Y) = & \E[(X - \E[X]) \cdot (Y - \E[Y])] \\
		=            & \E[X \cdot Y] - \E[X] \cdot \E[Y]
	\end{align*}
\end{definition}

\begin{definition}
	Se $\Var(X) \neq 0$ e $\Var(Y) \neq 0$, si chiama \textbf{coefficiente di correlazione} la
	quantità
	\[
		\rho(X,Y) = \frac{\Cov(X, Y)}{\sigma(X) \cdot \sigma(Y)}
		= \frac{\Cov(X, Y)}{\sqrt{\Var(X) \cdot \Var(Y)}}
	\]
	Possiamo dire che quando $\rho(X,Y) = 0$ le due variabili $X$ e $Y$ sono \emph{scorrelate}.
\end{definition}

Di seguito elenchiamo alcune proprietà molto utili per la covarianza
\begin{itemize}
	\item $\Cov(aX + bY + c, Z) = a \cdot \Cov(X, Z) + b \cdot \Cov(Y, Z)$
	\item $\Cov(X,Y) = \Cov(Y,X)$ e $\Var(X) = \Cov(X, X)$
	\item $\Var(X+Y) = \Var(X) + \Var(Y) + 2 \cdot \Cov(X, Y)$
	\item $\rho(X,Y) = 0 \implies \Var(X + Y) = \Var(X) + \Var(Y)$
\end{itemize}

\begin{proposition}
	Date $X$ e $Y$ con momento secondo e tali che $\Var(X) \neq 0$ e $\Var(Y) \neq 0$, allora
	\begin{itemize}
		\item $|\rho(X,Y)| \leq 1$
		\item $\min_{(a,b) \in \R^2} (\E[(Y - a - bX)^2]) = \Var(Y) \cdot (1 - \rho(X,Y)^2)$
	\end{itemize}
\end{proposition}

Per capire meglio il secondo punto dobbiamo considerare $(a^*,b^*)$ che realizzano il minimo
nella seconda equazione, la retta $y = a^* + b^* X$ è la migliore approssimazione lineare tra
$X$ e $Y$. Stiamo cioè minimizzando la distanza tra $Y$ e $a+bX$. Il valore del minimo è
proporzionale a $1 - \rho(X,Y)^2$.

In altre parole, la relazione tra $X$ e $Y$ è tanto meglio approssimata dalla retta $y=a^*+b^*X$
quanto più $\rho$ è vicino a 1. In questo senso $rho$ è una misura della \textbf{dipendenza lineare}
tra $X$ e $Y$.

A questo punto dobbiamo stare attenti a non confonderci con l'indipendenza: se due variabili sono
indipendenti allora avranno una dipendenza lineare nulla $\rho=0$, ma in generale non è vero che
se $\rho=0$ allora le due variabili sono indipendenti (potrebbe esserci un'altra forma di
dipendenza).

\begin{proposition}
	Se $X$ e $Y$ sono indipendenti e hanno momento secondo, allora sono scorrelate. Il viceversa è
	in generale falso.
	\begin{proof}
		Se $X$ e $Y$ sono indipendenti vale
		\[ \Cov(X,Y) = \E[X \cdot Y] - \E[X] \cdot \E[Y] = 0 \]
		Per il viceversa serve un controesempio e nel nostro caso possiamo considerare
		$X \sim U([0,1])$ e $Y = X^2$. Si può verificare che $X$ e $Y$ sono scorrelate ma non sono
		indipendenti.
	\end{proof}
\end{proposition}

\subsection{Valore atteso e varianza di variabili notevoli}
Passiamo ora alla formulazione specifica di valore atteso e varianza per le variabili aleatorie
notevoli di cui abbiamo parlato in precedenza.

\subsubsection{Variabili binomiali}
Nel caso di una variabile aleatoria $X \sim B(n,p)$ abbiamo che, nel caso $n=1$, vale
\[
	X = \begin{cases}
		1 & \text{successo}   \\
		0 & \text{fallimento}
	\end{cases}
\]
In questo caso, il valore atteso vale
\begin{align*}
	\E[X] = & 1 \cdot P(X = 1) + 0 \cdot P(X = 0) \\
	=       & 1 \cdot p + 0 \cdot (1-p) = p
\end{align*}
Per quanto riguarda la varianza calcoliamo prima
\[ \E[X^2] = 1^2 \cdot P(X=1) + 0^2 \cdot P(X = 0) = p \]
La varianza risulta quindi essere
\[ \Var(X) = \E[X^2] - \E[X]^2 = p - p^2 = p(1-p) \]
Nel caso con $n \in \N^+$ abbiamo $X \sim B(n,p)$ e quindi $X$ equivale al numero di successi in
$n$ prove ripetute. Un modo semplice di vederlo è il seguente
\[ X = X_1 + X_2 + \dots + X_n \]
dove le variabili $X_i \sim B(p)$ sono indipendenti e quindi il valore atteso si scrive come
\begin{align*}
	\E[X] = & \E[X_1 + X_2 + \dots + X_n]         \\
	=       & \E[X_1] + \E[X_2] + \dots + \E[X_n] \\
	=       & n \cdot p
\end{align*}
Per quanto riguarda la varianza abbiamo che
\begin{align*}
	\Var(X) = & \Var(X_1 + X_2 + \dots + X_n)             \\
	=         & \Var(X_1) + \Var(X_2) + \dots + \Var(X_n) \\
	=         & n \cdot p \cdot (1-p)
\end{align*}

\subsubsection{Variabili di Poisson}
Nel caso di una variabile aleatoria $X \sim P(\lambda)$ con $\lambda > 0$ abbiamo che
\[ P_X (k) = \frac{\lambda^k}{k!} \cdot e^{-\lambda} \]
con $k \in \N$. Dato che $X \geq 0$ esiste il valore atteso $\E[X] \in [0, +\infty)$ ed è
rappresentato dalla quantità
\begin{align*}
	\E[X] = & \sum_{k \in \N} k \cdot P(X = k)                                                   \\
	=       & \sum_{k=0}^{+\infty} k \cdot \frac{\lambda^k}{k!} e^{-\lambda}                     \\
	=       & \sum_{i=1}^{+\infty} \lambda \cdot \frac{\lambda^{k-1}}{(k-1)!} \cdot e^{-\lambda} \\
	=       & \lambda \sum_{i=1}^{+\infty} \cdot \frac{\lambda^{k-1}}{(k-1)!} \cdot e^{-\lambda} \\
	=       & \lambda
\end{align*}
Calcoliamo ora la varianza tenendo di conto che $\E[X^2] = \lambda^2 + \lambda$
\[ \Var(X) = \E[X^2] - \E[X]^2 = \lambda^2 + \lambda - \lambda^2 = \lambda \]

\subsubsection{Variabili uniformi}
Consideriamo ora variabili aleatorie con densità partendo dalle uniformi e facendo un'osservazione
preliminare

\begin{observation}
	Se $X$ ha densità $f$ con $f = 0$ fuori da un intervallo limitato $I$, allora $X$ ha momenti
	di ogni ordine.
\end{observation}

Possiamo quindi dire che $X \sim U([a,b])$ ammette momenti di ogni ordine e il valore atteso è
rappresentato dalla quantità
\begin{align*}
	\E[X] = & \int_{-\infty}^{+\infty} x \cdot f (x) dx           \\
	=       & \int_a^b x \cdot \frac{1}{b-a} dx                   \\
	=       & \frac{1}{b - a} \cdot \frac{1}{2} x^2 \Big|_{x=a}^b \\
	=       & \frac{1}{b - a} \cdot \frac{1}{2} \cdot (b^2 - a^2) \\
	=       & \frac{a + b}{2}
\end{align*}
Calcoliamo ora
\begin{align*}
	\E[X^2] = & \int_a^b x^2 \cdot \frac{1}{b-a} dx \\
	=         & \frac{b^3 - a^3}{3 \cdot (b-a)}     \\
	=         & \frac{a^2 + ab + b^2}{3}
\end{align*}
Calcoliamo quindi la varianza
\[ \Var(X) = \E[X^2] - \E[X]^2 = \frac{(b - a)^2}{12} \]

\subsubsection{Variabili esponenziali}
Data $X \sim E(\lambda)$ con $\lambda > 0$ abbiamo che la sua densità è
\[
	f(x) = \begin{cases}
		\lambda e^{-\lambda x} & x > 0    \\
		0                      & x \leq 0
	\end{cases}
\]
\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
				width=8cm,
				height=5cm,
				axis lines = center,
				xlabel = $x$,
				ylabel = $y$,
				ticks = none,
				ymin = -0.2, ymax = 1.2,
				xmin = -1, xmax = 5,
				grid = none,
				enlargelimits
			]
			\addplot [domain=0:6, samples=50, color=red, thick] {exp(-x)};
			\addplot [domain=-2:0, color=red, thick] {0};
		\end{axis}
	\end{tikzpicture}
\end{center}
Come possiamo notare sia dalla formula che dal grafico, $f$ non è limitata ed è non nulla per tutta
la semiretta positiva. Non possiamo quindi dire a priori quali momenti ammette $X$.

Possiamo però dire che $f = 0$ per $x \leq 0$, quindi $X > 0$ con probabilità 1, quindi esiste
$\E[X] \in [0, +\infty)$ ed è rappresentato dalla quantità
\begin{align*}
	\E[X] = & \int_{-\infty}^{+\infty} x \cdot f(x) dx                      \\
	=       & \int_0^{+\infty} x \cdot \lambda \cdot e^{-\lambda x} dx      \\
	=       & -x \cdot e^{-\lambda x} \Big|_{x=0}^{+\infty} -
	\int_{0}^{+\infty} (-e^{-\lambda x}) dx                                 \\
	=       & 0 - 0 - \frac{1}{\lambda} e^{\lambda x} \Big|_{x=0}^{+\infty} \\
	=       & \frac{1}{\lambda}
\end{align*}
\`E immediato notare che $\E[X^2] = 2 / \lambda^2$ e quindi la varianza equivale a
\[ \Var(X) = \E[X^2] - \E[X]^2 = \frac{1}{\lambda^2} \]

\subsubsection{Variabili Gaussiane}
Come ultimo caso consideriamo le variabili Gaussiane partendo dal caso di una variabile Gaussiana
standard. Sia quindi $X \sim N(0,1)$ con densità
\[ f(x) = \frac{1}{\sqrt{2 \pi}} \cdot e^{-x^2 / 2} \]
\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
				width=10cm,
				height=5cm,
				axis lines = center,
				xlabel = $x$,
				ylabel = $y$,
				ticks = none,
				ymin = -0.2, ymax = 0.5,
				xmin = -4, xmax = 4,
				grid = none,
				enlargelimits
			]
			\addplot [
				domain=-4:4,
				samples=100,
				color=red,
				thick
			]
			{(exp(-x^2 / 2) / sqrt(2 * 3.14))};
		\end{axis}
	\end{tikzpicture}
\end{center}
Similmente al caso precedente abbiamo che $f$ è non nulla su tutto $\R$ e quindi non possiamo dire
se $X$ ammette momenti. Possiamo però calcolare
\[
	\E[|X|^n] = \int_{-\infty}^{+\infty} |x|^n \cdot
	\frac{1}{\sqrt{2 \pi}} \cdot e^{-x^2 / 2} dx \leq e^{x^2 / 4}
\]
per $x \geq M_n$ dove $M_n$ è un valore che dipende da $n$. Risparmiandoci i passaggi matematici
possiamo dire che $|x|^n \cdot e^{-x^2 / 4} \leq 1$, è dunque limitato e quindi ammette momenti di
ogni ordine.

Possiamo quindi calcolare il valore atteso rappresentato dalla seguente formula
\[ \E[X^n] = \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi}} \cdot x^n \cdot e^{-x^2 / 2} dx \]
Dobbiamo però analizzare due casi distinti. Se $n$ è dispari otteniamo una cosa di questo tipo
\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
				width=10cm,
				height=5cm,
				axis lines = center,
				xlabel = $x$,
				ylabel = $y$,
				ticks = none,
				ymin = -0.5, ymax = 0.5,
				xmin = -4, xmax = 4,
				grid = none,
				enlargelimits
			]
			\addplot [
				domain=-4:4,
				samples=100,
				color=red,
				thick
			]
			{x * exp(-x^2 / 2) / sqrt(2 * 3.14)};
		\end{axis}
	\end{tikzpicture}
\end{center}
per la quale abbiamo una proprietà di simmetria tale che
\[ \int_{0}^{+\infty} x^n \cdot e^{-x^2/2} dx = - \int_{-\infty}^{0} x^n \cdot e^{-x^2/2} dx \]
e quindi, per $n$ dispari abbiamo che
\[ \E[X] = \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi}} \cdot x^n \cdot e^{-x^2/2} dx = 0 \]
Possiamo quindi dire che tutti i momenti dispari sono 0 e quindi anche il valore atteso. Nel caso
in cui $n$ sia pari abbiamo questa situazione
\begin{center}
	\begin{tikzpicture}
		\begin{axis}[
				width=10cm,
				height=5cm,
				axis lines = center,
				xlabel = $x$,
				ylabel = $y$,
				ticks = none,
				ymin = -0.2, ymax = 0.5,
				xmin = -4, xmax = 4,
				grid = none,
				enlargelimits
			]
			\addplot [
				domain=-4:4,
				samples=100,
				color=red,
				thick
			]
			{x^2 * exp(-x^2 / 2) / sqrt(2 * 3.14)};
		\end{axis}
	\end{tikzpicture}
\end{center}
e il momento secondo vale
\begin{align*}
	\E[X^2] = & \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} x^2 \cdot e^{-x^2/2} dx    \\
	=         & \frac{2}{\sqrt{2 \pi}} \int_{0}^{+\infty} x^2 \cdot e^{-x^2/2} dx          \\
	=         & \frac{2}{\sqrt{2 \pi}} \cdot x \cdot (-e^{-x^2/2}) \Big|_{x=0}^{+\infty} -
	\frac{2}{\sqrt{2 \pi}} \int_{0}^{+\infty} 1 \cdot (-e^{x^2/2}) dx                      \\
	=         & 0 + \frac{2}{\sqrt{2 \pi}} \int_{0}^{+\infty} e^{-x^2/2} dx                \\
	=         & \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} e^{-x^2/2} dx = 1
\end{align*}
Abbiamo quindi $\E[X^2] = 1$ e di conseguenza la varianza vale
\[ \Var(X) = \E[X^2] - \E[X]^2 = 1 - 0 = 1 \]
Consideriamo ora il caso di una generica Gaussiana di parametri $\mu$ e $\sigma^2$ indicata come
$X \sim N(\mu, \sigma^2)$. Come abbiamo visto, possiamo scrivere $X$ come
\[ X = \sigma Y + \mu \]
con $Y \sim N(0,1)$. In questo caso il valore atteso di $X$ equivale a
\[ \E[X] = \sigma \cdot \E[Y] + \mu = \mu \]
Calcoliamo ora la varianza in questo modo
\[ \Var(X) = \Var(\sigma \cdot Y + \mu) = \sigma^2 \cdot \Var(Y) = \sigma^2 \]
Come possiamo notare, per le variabili Gaussiane, i parametri $\mu$ e $\sigma^2$ sono
rispettivamente valore atteso e varianza.