\section{Probabilità condizionata}
La \textbf{probabilità condizionata} indica la probabilità che si verfichi un evento sapendo che
se ne verifica un altro.

\begin{example}
	Supponiamo di lanciare un dado equilibrato e supponiamo di sapere che l'esito è un numero
	pari. Sapendo ciò, vogliamo sapere quale sia la probabilità che l'esito sia maggiore o uguale
	di 4. Consideriamo i due insiemi $A$ e $B$ così definiti.
	\[ A = \{ 4, 5, 6 \} \quad B = \{ 2, 4, 6 \} \]
	Vogliamo sapere qual'è la probabilità di estrarre dall'insieme $B$ un numero maggiore o uguale
	di 4. \`E immediato che la risposta debba essere $2/3$ dato che
	\[
		\frac{2}{3} = \frac{\# (A \cap B)}{\# B} =
		\frac{\# (A \cap B) / \# \Omega}{\# B / \# \Omega} =
		\frac{P(A \cap B)}{P(B)}
	\]
\end{example}

\begin{definition}
	Dato uno spazio di probabilità $(\Omega, \F, P)$ e un evento non trascurabile $B$, definiamo
	\textbf{probabilità condizionata} di $A$ dato $B$, come
	\[ P(A | B) = \frac{P(A \cap B)}{P(B)} \]
	Essa indica la probabilità che accada $A$ sapendo che accade $B$.
\end{definition}

Dalla formula segue che se $A$ e $B$ sono eventi e $B$ è non trascurabile, allora
\[ P(A \cap B) = P(A | B) \cdot P(B) \]
e vale una formula più generale data dalla seguente proposizione.

\begin{proposition}
	Siano $A_1, A_2, \dots, A_n$ eventi la cui intersezione è non trascurabile, allora
	\[
		P(A_1 \cap  \dots \cap A_n) = P(A_1) P(A_2 | A_1) P(A_3 | A_1 \cap A_2)
		\dots P(A_n | A_1 \cap \dots \cap A_{n-1})
	\]
\end{proposition}

\begin{definition}
	Siano $B_1, \dots, B_n$ eventi non trascurabili che formano una partizione dello spazio
	campionario $\Omega$, se
	\[ B_i \cap B_j = \emptyset \quad \land \quad B_1 \cup \dots \cup B_n = \Omega \]
	per ogni $i,j$ con $i \neq j$, allora sono detti \textbf{sistema di alternative}.
\end{definition}

\begin{theorem}[Fattorizzazione]\label{th: fattorizzazione}
	Sia $B_1, \dots, B_n$ un sistema di alternative, allora vale
	\[ P(A) = \sum_{i=1}^n P(A | B_i) \cdot P(B_i) \]
	Per ogni evento $A \in \F$.
	\begin{proof}
		Se partizioniamo $\Omega$ in $n$ alternative e poi definiamo un insieme
		$A \subseteq \Omega$, possiamo vedere $A$ in questo modo
		\[ A = \cup_{i=1}^n (A \cap B_i) \]
		ossia come l'unione disgiunta dei pezzi di $A$ che stanno nei vari $B_i$. Quindi vale che
		\[ P(A) = \sum_{i=1}^n P(A \cap B_i) = \sum_{i=1}^n P(A | B_i) P(B_i) \]
	\end{proof}
\end{theorem}

Generalmente questa formula si applica in casi in cui $P$ non è nota a priori ma sono note le
probabilità condizionate relative ad un sistema di alternative.

\begin{example}
	Ci sono 2 urne, la prima con 5 biglie rosse e 5 blu, la seconda con 8 biglie rosse e 2 blu.
	Scegliamo casualmente una delle due urne e da questa estraiamo una biglia e ne osserviamo il
	colore. Vogliamo calcolare la probabilità che esca una biglia rossa. In questo esempio lo
	spazio campionario è il seguente
	\[ \Omega = \{ (\text{urna}, \text{colore biglia}) \} \]
	dove
	\[ \text{urna} \in \{ 1, 2 \} \quad \text{colore biglia} \in \{ r, b \} \]
	A questo punto noi sappiamo che la probabilità di scegliere una delle due urne è
	\[ P(1) = P(2) = \frac{1}{2} \]
	La seconda informazione di cui siamo a conoscienza è che se scegliamo l'urna 1 abbiamo 5
	biglie rosse e 5 blu e quindi
	\[ P(r | 1) = \frac{5}{10} = \frac{1}{2} \]
	Se invece scegliamo l'urna 2 abbiamo 8 biglie rosse e quindi
	\[ P(r | 2) = \frac{8}{10} = \frac{4}{5} \]
	A questo punto siamo in grado di calcolare la probabilità complessiva di estrarre una biglia
	rossa ricavandola con il teorema di fattorizzazione \ref{th: fattorizzazione} in questo modo
	\[ P(r) = P(r | 1) \cdot P(1) + P(r | 2) \cdot P(2) = \frac{13}{20} \]
\end{example}

\begin{example}
	Un test diagnostico per una data malattia ha un indice di sensibilità di $0.99$ per una
	persona malata, ossia, da esito positivo con probabilità $0.99$. E ha indice di specificità
	$0.97$, ossia, da esito negativo con probabilità $0.97$ per una persona sana.

	Supponendo che l'1\% della popolazione soffra di tale malattia, vogliamo calcolare la
	probabilità che il test, su un individuo a caso, dia esito positivo. Si applica la formula di
	fattorizzazione al sistema di alternative $B_1 = \{ \text{sano} \}$ e
	$B_2 = \{ \text{malato} \}$ e all'evento $A = \{ \text{test positivo} \}$. Svolgendo i calcoli
	in modo analogo a prima otteniamo che
	\[ P(A) = 0.0396 \]
\end{example}

\begin{theorem}[Bayes]\label{th: bayes}
	Siano $A$ e $B$ eventi non trascurabili. Allora vale
	\[ P(B | A) = \frac{P(A | B) \cdot P(B)}{P(A)} \]
	In particolare se $B_1, B_2, \dots, B_n$ allora vale
	\[
		P(B_i | A) = \frac{P(A | B_i) \cdot
			P(B_i)}{\displaystyle\sum_{j=1}^n P(A | B_j) \cdot P(B_j)}
	\]
	per ogni $i = 1, \dots, n$.
	\begin{proof}
		La dimostrazione è molto semplice basta infatti notare che
		\[
			P(B | A) = \frac{P(B \cap A)}{P(A)} =
			\frac{P(A \cap B)}{P(A)} = \frac{P(A | B) \cdot P(B)}{P(A)}
		\]
		La seconda formula segue dalla prima dove $B=B_i$ e dalla formula di fattorizzazione
		per $P(A)$.
	\end{proof}
\end{theorem}

La formula di Bayes viene usata per \emph{invertire} il condizionamento tipicamente in contesti
in cui accade un evento $A$ riferito a un'\emph{osservabile} e vogliamo dedurre la probabilità di
un'alternativa $B_i$ di eventi \emph{causa}.

\begin{example}
	Facendo riferimento all'esempio di prima delle biglie, supponiamo di estrarre una biglia
	rossa e vogliamo calcolare la probabilità che questa sia stata estratta dalla prima urna.
	\[
		P(1 | r) = \frac{P(r | 1) P(1)}{P(r)} =
		\frac{\frac{5}{10} \cdot \frac{1}{2}}{\frac{13}{20}} = 0.385
	\]
\end{example}

\begin{example}
	Facendo riferimento al test diagnostico, supponiamo che il test dia esito positivo e vogliamo
	calcolare la probabilità che la persona sia effettivamente malata.
	\[
		P(\text{malato} | \text{positivo}) =
		\frac{P(\text{positivo} | \text{malato}) \cdot
			P(\text{malato})}{P(\text{positivo})} = 0.25
	\]
\end{example}

Come possiamo notare, si fa riferimento a casi in cui la probabilità condizionata nel membro di
destra dell'equazione è già nota.