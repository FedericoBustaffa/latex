\chapter{Sequenze casuali}
Introdurre le sequenze casuali ci permette di alimentare \textbf{algoritmi randomizzati} e sono inoltre molto utili
per \textbf{generare le chiavi} di cifratura e decifrazione nel modo migliore possibile.

Prendiamo due sequenze di esempio $h_1$ e $h_2$, entrambe lunghe 20 cifre
\[
	\begin{matrix}
		h_1 = 1 & 1 & 1 & 1 & 1 & \dots & 1 & \dots & 1 \\
		h_2 = 0 & 1 & 1 & 0 & 0 & \dots & 0 & \dots & 0
	\end{matrix}
\]
facciamo finta di aver ottenuto entrambe le sequenze lanciando 20 volte una moneta. La probabilit\`a di ottenere sia
$h_1$ che $h_2$ con questo metodo \`e di $\frac{1}{2}^{20}$.

Se dovessimo indicare quale delle due \`e quella \emph{casuale} sarebbe naturale indicare $h_2$. Ma definiamo formalmente
cosa sia la \emph{casualit\`a} ?

Il matematico russo Kolmogorov ha descritto il \textbf{significato algoritmico di casualit\`a} per una sequenza binaria.

\begin{definition}[Kolmogorov]
	Una sequenza \emph{binaria} $h$ \`e \textbf{casuale} se non ammette un algoritmo $A$, in grado di descrivere $h$, la
	cui rappresentazione binaria sia pi\`u corta di $h$. Quindi $h$ \`e casuale se
	\[ |h| \leq |A_h| \]
	ovvero possiamo affermare che una sequenza non \`e casuale se c'\`e un algoritmo \emph{semplice} che la descrive.
\end{definition}

\begin{example}
	Prendiamo ora come esempio una sequenza $h$ di $n$ 1. Abbiamo che
	\[ |h| = n \]
	e un algoritmo $A_h$ che la descrive
	\begin{lstlisting}
	for(int i = 0; i < n; i++)
		print(1);
	\end{lstlisting}
	La lunghezza della rappresentazione in binario di $A_h$ avr\`a lunghezza
	\[ |A_h| = \text{cost} + \theta(\log n) \]
	dato che per rappresentare $n$ in binario ho bisogno di $\log n$ bit. Abbiamo quindi descritto con $\log n$	bit
	una sequenza di $n$ bit.
\end{example}

\begin{example}
	Considerando invece una sequenza $h$ che ci appare casuale e non presenta evidenti regolarit\`a, l'algoritmo di
	rappresentazione deve contenerla interamente al suo interno e la restituisce in output.
	\begin{lstlisting}
	print(1001011110...1010...);
	\end{lstlisting}
	In questo caso la lunghezza della rappresentazione in binario di $A_h$ avr\`a lunghezza
	\[ |A_h| = \text{cost} + \theta(n) \]
	dato che non ho un modo compatto per descrivere $h$.
\end{example}

\section{Sistemi di calcolo}
Iniziamo ora a mettere in relazione le sequenze con i \textbf{sistemi di calcolo} dai quali vengono generate.

Prima di addentrarci nell'argomento facciamo qualche precisazione. I sistemi di calcolo sono certamente \emph{infiniti}
ma anche \emph{numerabili} dato che devo poterli descrivere con sequenze finite di caratteri di un alfabeto finito.

\begin{definition}[Complessit\`a di Kolmogorov]
	Sia $h$ una sequenza, $S_i$ un generico sistema di calcolo e $p$ un programma scritto nel formalismo richiesto dal
	sistema di calcolo $S_i$. Indico con
	\[ K_{S_i}(h) = \min\{ |p| \mid S_i(p) = h \} \]
	la \textbf{complessit\`a di Kolmogorov}, ossia la lunghezza del programma pi\`u breve in grado di generare la
	sequenza $h$ nel sistema $S_i$.
\end{definition}

\subsection{Sistema di calcolo universale}
Ci\`o che vogliamo fare \`e rendere la definizione di casualit\`a indipendente dal sistema di calcolo adottato.

Tra i sistemi di calcolo esiste almeno un sistema di calcolo \textbf{universale} $S_u$ in grado di simulare tutti gli
altri sistemi di calcolo.

Questo sistema di calcolo prende in input una coppia $q$, formata da un indice $i$ (l'indice del sistema di calcolo da
andare a simulare) e un programma $p$ da far girare sul sistema di calcolo $S_i$.
\[ S_u(q) =  S_u (\langle i, p \rangle) = S_i (p) = h \]
Se andiamo a valutare la lunghezza di $q$ in bit otteniamo che
\[ |q| = |i| + |p| = \theta (\log i) + |p| \]
dato che per rappresentare $i$ ho bisogno di $\log i$ bit. Notiamo anche che $|i|$ \`e un termine che dipende unicamente
dal sistema di calcolo e non dalla sequenza $h$.

Proviamo ora a valutare la complessit\`a di Kolmogorov su $S_u$. Possiamo dire certamente che
\[ K_{S_u}(h) \leq K_{S_i}(h) + c_i \]
dove $c_i$ \`e una costante che non dipende da $h$.

\begin{definition}
	La \textbf{complessit\`a} secondo Kolmogorov di una sequenza $h$ \`e
	\[ K(h) = K_{S_u}(h) \]
	infatti
	\[ \forall S_i \quad K_{S_i}(h) \geq K_{S_u}(h) \]
	a meno di una costante.
\end{definition}

\begin{definition}[Casualit\`a secondo Kolmogorov]
	Una sequenza $h$ \`e \textbf{casuale} se
	\[ K(h) \geq |h| - \lceil \log_2 |h| \rceil \]
	La ragione per cui si sottrae un fattore logaritmico \`e per rilassare il vincolo di casualit\`a, altrimenti
	soddisfatto da ben poche sequenze.
\end{definition}

Notiamo che la casualit\`a \`e una \emph{propriet\`a} della sequenza, e non dipende quindi dal sistema di calcolo
che l'ha generata.

\section{Esistenza}
Dimostriamo adesso l'\textbf{esistenza} di sequenze casuali secondo la teoria appena descritta.

\begin{proof}
	Consideriamo solo sequenze binarie lunghe $n$ bit e indichiamo con $S$ il numero di sequenze binarie lunghe $n$.
	\[ S = 2^n \]
	Indichiamo inoltre con $T$ il numero di sequenze binarie lunghe $n$ \textbf{non casuali} secondo la definizione data
	in precedenza.

	Per dimostrare l'esistenza di sequenze casuali, devo mostrare che $T < S$. Devo quindi dimostrare che l'insieme $S$
	non \`e composto esclusivamente da sequenze non casuali. Di conseguenza, le sequenze in $S$, che non sono in $T$
	devono essere necessariamente casuali.

	Iniziamo con il ricordare una caratteristica detta in precedenza sulle sequenze in $T$ e quindi non casuali. I
	programmi che generano sequenze lunghe $n$ in $T$ hanno lunghezza
	\[ |p| < n - \lceil \log_2 n \rceil \]
	perch\'e se la lunghezza \`e almeno $n - \lceil \log_2 n \rceil$ allora la sequenza \`e casuale.

	Indichiamo con $N$ il numero di sequenze binarie $h$ tali che
	\[ |h| < n - \lceil \log_2 n \rceil \]
	Tra queste ci saranno anche le sequenze che codificano i programmi che generano le $T$ sequenze non casuali. Possiamo
	ora definire $N$ come segue
	\[ N = \sum_{i=0}^{n - \lceil \log_2 n \rceil - 1} 2^i = 2^{n - \lceil \log_2 n \rceil} - 1 \]
	Se mettiamo a confronto $N$ con $S$ notiamo subito che
	\[ N < S \]
	ma abbiamo che in $N$ ci sono tutti i programmi che generano le $T$ sequenze non casuali, quindi
	\[ T \leq N < S \quad \Rightarrow \quad T < S \]
\end{proof}

Possiamo fare un'altra considerazione. Se sviluppiamo il seguente limite
\[ \lim_{n \rightarrow +\infty} \frac{T}{S} \]
otteniamo
\[
	\lim_{n \rightarrow +\infty} \frac{2^{n - \lceil \log n \rceil} - 1}{2^n} =
	\lim_{n \rightarrow +\infty} \left( \frac{1}{2^{\lceil \log n \rceil}} - \frac{1}{2^n}\right) = 0
\]
Questo ci dice che al crescere di $n$ le sequenze non casuali sono molte meno di quelle casuali.

Fatta questa considerazione ci potrebbe venire in mente di prendere una sequenza qualsiasi lunga $n$ e vedere se \`e
casuale. Purtroppo non esiste un algoritmo che, data una sequenza arbitraria, sia in grado di dirci se questa sia
casuale o meno secondo Kolmogorov.

\begin{theorem}
	Il problema di stabilire se una sequenza sia casuale secondo Kolmogorov \`e indecidibile.
	\begin{proof}
		Procediamo con una dimostrazione per assurdo. Ipotizziamo che esista un algoritmo
		\begin{lstlisting}[style=pseudo-style]
RANDOM(h)
	return h.isRandom();
		\end{lstlisting}
		Costruiamo a questo punto l'algoritmo \verb|PARADOSSO| che enumera tutte le sequenze di lunghezza $h$, chiama
		\verb|RANDOM| su ognuna di esse	e infine restituisce la prima sequenza casuale $h$ tale che
		\[ |h| - \lceil \log_2 |h| \rceil > 1 \]
		\begin{lstlisting}[style=pseudo-style]
PARADOSSO()
	for h = 1 to +inf
		if |h| - ceil(log(|h|)) > |P| and RANDOM(h) then
			return h;
		\end{lstlisting}
		\verb|P| \`e una stringa che rappresenta la codifica in binario di \verb|PARADOSSO| e di \verb|RANDOM|, vale
		quindi che
		\begin{center}
			|\verb|P|| \verb|=| |\verb|PARADOSSO|| \verb|+| |\verb|RANDOM||
		\end{center}
		e |\verb|P|| \`e una costante che non dipende da $h$. Questo perch\'e $h$ compare in \verb|PARADOSSO| solo come
		nome di variabile.

		\begin{enumerate}
			\item La prima condizione \`e vera quando incontro una sequenza $h$ tale che
			      \[ |h| - \lceil \log_2 |h| \rceil > |P| \]
			      ma \verb|P| \`e un programma \textbf{breve} che \emph{genera} $h$, ossia
			      \[ |P| < |h| - \lceil \log_2 |h| \rceil \]
			      ma questo equivale a dire che $h$ non \`e casuale secondo Kolmogorov.
			\item La seconda condizione \`e vera quando \verb|RANDOM| trova una sequenza casuale e, come detto in
			      precedenza, le sequenze casuali esistono e quindi prima o poi \verb|RANDOM| ne trover\`a una.
		\end{enumerate}
		Ecco che qui si ottiene una \textbf{contraddizione} dato che il programma termina su una sequenza che viene
		riconosciuta allo stesso tempo come casuale e come non casuale.
	\end{proof}
\end{theorem}

\section{Sorgente casuale binaria}
Iniziamo ora a parlare del come queste sequenze casuali vengono generate.

\begin{definition}
	Una \textbf{sorgente casuale binaria} produce un flusso di bit con due propriet\`a fondamentali:
	\begin{itemize}
		\item Lo 0 e l'1 si presentano con uguale probabilit\`a
		      \[ \begin{matrix} P(0) & = & P(1) & = & \displaystyle\frac{1}{2} \end{matrix} \]
		      quest'ipotesi, in realt\`a, non \`e cos\`i rigida. Non \`e necessario che le due probabilit\`a siano
		      uguali, basta che
		      \[ P(0) > 0 \quad P(1) > 0 \]
		      ma devono essere \textbf{immutabili} durante il processo. Quello che per\`o si fa per far comparire gli 0
		      gli 1 con uguale frequenza \`e generare la sequenza con probabilit\`a $P(0)$ e $P(1)$ diverse fra loro.
		      Per esempio prendiamo questa sequenza la cui sorgente ha $P(1) > P(0)$
		      \[ \begin{matrix} 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 1 \end{matrix} \]
		      a questo punto prendiamo in considerazione le coppie di bit generate
		      \[ \begin{matrix} 00 & 10 & 01 & 11 & 11 \end{matrix} \]
		      eliminiamo le coppie in cui i bit sono uguali e valutiamo come 0 le coppie 01 e come 1 le coppie 10,
		      otteniamo cos\`i la sequenza
		      \[ \begin{matrix} 1 & 0 \end{matrix} \]
		      Eliminiamo le coppie in cui i bit sono uguali perch\'e le coppie 11 appaiono con maggiore probabilit\`a
		      e le coppie 00 con minor probabilit\`a. Al contrario le coppie 01 e le coppie 10 sono equiprobabili.
		\item La generazione di un bit \`e indipendente da quella dei bit precedenti. Quest'ipotesi \`e al contrario
		      molto pi\`u stringente e non \`e neanche chiaro se sia davvero possibile soddisfarla.
	\end{itemize}
\end{definition}

\subsection{Generatori pseudocasuali}
Dato che riuscire a generare numeri casuali cercando di soddisfare la seconda ipotesi descritta in precedenza \`e molto
complesso, quello che si fa nella pratica \`e utilizzare \textbf{generatori pseudocasuali}, ossia generatori che generano
casualit\`a mediante un algoritmico, ricercandola all'interno di processi matematici.

Questi generatori sono definiti pseudocasuali perch\'e in genere sono programmi brevi e che quindi generano sequenze
non casuali secondo Kolmogorov.

Un altro motivo \`e che il numero di sequenze possibili dipende da un parametro di input ossia il \textbf{seme}. Le
possibili sequenze generate sono funzione della lunghezza del seme e sono molte meno di quelle generabili in realt\`a.

Il problema di questi generatori \`e che producono un flusso di bit che ad un certo punto si ripete. Questo avviene
perch\'e, quando, in modo casuale il seme iniziale verr\`a rigenerato, la sequenza ripartir\`a da capo.

La sequenza che viene ripetuta all'interno del flusso \`e detta \textbf{periodo}.

Il seme lo otteniamo con un procedimento casuale (per esempio prendendo lo stato interno dell'orologio del calcolatore)
e il periodo lo otteniamo in modo deterministico a partire dal seme.

Possiamo pensare infine al generatore pseudocasuale come ad un \emph{amplificatore} di casualit\`a dato che cerca di
rendere il periodo pi\`u lungo possibile.

Un generatore \`e tanto migliore quanto pi\`u lungo \`e il periodo che riesce a generare.

Precisiamo che, essendo deterministico, il generatore ha bisogno sempre di un seme diverso per generare una sequenza
diversa. Se forniamo sempre il solito seme otteniamo sempre la solita sequenza.

Supponiamo di partire da un seme $S$ di $s$ bit. Il generatore genera al pi\`u $2^s$ sequenze diverse lunghe quanto il
periodo del generatore.

\subsubsection{Generatore lineare}
\`E un generatore molto semplice che sfrutta la funzione
\[ x_i = (a x_{i - 1} + b) \mod{m} \]
per la generazione del flusso di bit. I parametri $a$, $b$ ed $m$ sono interi positivi.

Il generatore avr\`a periodo $\leq m$ dato che, lavorando modulo $m$, posso ottenere al massimo $m$ sequenze diverse.
Nella pratica i parametri $a$ e $b$ si scelgono appositamente per riuscire a far s\`i che il periodo sia esattamente
uguale ad $m$ e non minore.

Se i parametri sono scelti bene il generatore produce una permutazione degli interi da 0 a $m - 1$. I criteri per fare
ci\`o sono
\begin{itemize}
	\item I parametri $b$ ed $m$ devono essere coprimi
	      \[ (b, m) = 1 \]
	\item Il parametro $a - 1$ deve essere divisibile per ogni fattore primo di $m$.
	\item Se $m$ \`e un multiplo di 4 allora anche $a-1$ deve esserlo
	      \[ 4 \mid m \quad \Rightarrow \quad 4 \mid a - 1 \]
\end{itemize}
Come possiamo vedere questo generatore non possiede la seconda propriet\`a fondamentale dato che un numero generato
dipende fortemente dal numero generato in precendenza.

Per ora abbiamo usato il generatore per la generazione di numeri interi ma se volessimo generare sequenze binarie
si fa $x_i / m$ e si prende la \textbf{parit\`a} della prima cifra decimale.

\subsubsection{Generatore polinomiale}
Il primo metodo per riuscire a \emph{generalizzare} un po' il generatore lineare \`e aumentare il grado del polinomio.
Riscriviamo quindi la funzione la funzione precedente ma con un polinomio di grado $t$
\[ x_i = (a_1 x_{i-1}^t + a_2 x_{i-1}^{t-1} + \dots + a_t x_{i-1} + a_{t+1}) \mod{m} \]
Per quanto si possa aumentare il grado del polinomio questo generatore non \`e tanto migliore del generatore lineare.

Anche per questo generatore dobbiamo scegliere bene i parametri per rendere il periodo pi\`u lungo possibile, ossia
uguale a $m$. Una scelta molto comune di tali parametri \`e questa:
\[
	\begin{matrix}
		a & = & 3141592653 \\
		b & = & 2718281829 \\
		m & = & 2^{32}
	\end{matrix}
\]
Come possiamo notare $a$ \`e $\pi$ e $b$ \`e $e$ (costante di Nepero).

\subsection{Test statistici}
Nel caso volessimo costruire il nostro generatore personale, per riuscire a capire quanto sia effettivamente valido o
ben costruito, dobbiamo sottoporlo ad alcuni \textbf{test statistici}:
\begin{itemize}
	\item \textbf{Test di frequenza}: si verifica che i diversi simboli della sequenza appaiano con pari probabilit\`a.
	\item \textbf{Poker test}: si controlla che le sottosequenze della stessa lunghezza siano equiprobabili.
	\item \textbf{Test di autocorrelazione}: si verifica che, a distanze fissate, non appaia sempre lo stesso simbolo.
	\item \textbf{Run test}: si verifica che sottosequenze composte dallo stesso simbolo siano esponenzialmente sempre
	      meno frequenti in relazione alla loro lunghezza.
\end{itemize}
Ovviamente per effettuare un buon test la sequenza deve essere sufficientemente lunga da permettere un'analisi esauriente.

Sia il generatore lineare che quello polinomiale superano questi test ma non vanno comunque bene per la generazione di
chiavi crittografiche. Anche tenendo nascosti i parametri, esistono algoritmi di costo polinomiale che sono in grado di
stimare con una probabilit\`a significativamente maggiore di $1/2$, quale sar\`a il prossimo bit generato.

\subsubsection{Test di prossimo bit}
Per le applicazioni crittografiche si richiede un test in pi\`u, il \textbf{test di prossimo bit}.

Un generatore binario supera il test di prossimo bit se non esiste un algoritmo polinomiale in grado di prevedere il
prossimo bit generato a partire dalla conoscenza dei bit gi\`a generati con probabilit\`a strettamente maggiore di $1/2$.

Se un generatore super il test di ultimo bit supera anche tutti gli altri test statistici ed \`e detto
\textbf{crittograficamente sicuro}.

\subsection{Generatori crittograficamente sicuri}
La costruzione di generatori sicuri si basa sulle cosiddette funzioni \textbf{one-way}, ossia funzioni \emph{facili} da
calcolare ma \emph{difficili} da invertire.
\begin{itemize}
	\item $y = f(x)$ si calcola in tempo polinomiale.
	\item $x = f^{-1}(y)$ si calcola in tempo esponenziale.
\end{itemize}
Per generare la sequenza si parte da un seme $x_0$ (mantenuto segreto) e si calcola in tempo polinomiale la sequenza
\[
	\begin{matrix}
		x_1   & = f(x_0)     &             &                \\
		x_2   & = f(x_1)     & = f(f(x_0)) & = f^{(2)}(x_0) \\
		\dots &              & \dots       &                \\
		x_i   & = f(x_{i-1}) & \dots       & = f^{(i)}(x_0) \\
		\dots &              & \dots       &
	\end{matrix}
\]
A questo punto per\`o dobbiamo consumarla al contrario. Questo perch\'e si deve sempre assumere che il crittoanalista
conosca $f$. Conoscendo $f$ e vedendo un generico $x_i$ \`e facile calcolare $f(x_{i+1})$.

Se invece consumo la sequenza al contrario, l'unico modo per il crittoanalista di prevedere il prossimo bit generato
\`e quello di invertire $f$, ma come abbiamo gi\`a detto, \`e un'operazione che richiede tempo esponenziale.

\subsubsection{Predicati hard-core delle funzioni one-way}
Ora vogliamo rendere il generatore appena descritto un generatore \emph{binario} che per\`o conservi le propriet\`a che
lo rendono crittograficamente sicuro.