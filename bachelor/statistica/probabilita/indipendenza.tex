\section{Indipendenza}
Il concetto di \textbf{indipendenza} nasce dalla necessità di esprimere in modo rigoroso che la
probabilità di un evento $A$ non cambia sapendo che accade $B$ e viceversa. Per $A$ e $B$ non
trascurabili vale quindi
\[ P(A) = P(A | B) \quad \Leftrightarrow \quad P(A) \cdot P(B) = P(A \cap B) \]
Lo stesso vale per $P(B)$.

\begin{definition}
	Due eventi $A$ e $B$ sono \textbf{indipendenti} se
	\[ P(A \cap B) = P(A) \cdot P(B) \]
\end{definition}

Si può dimostrare per esercizio che, se $A$ e $B$ sono indipendenti allora lo sono anche
\begin{itemize}
	\item $A^c$ e $B$
	\item $A$ e $B^c$
	\item $A^c$ e $B^c$
\end{itemize}
L'indipendenza è dunque stabile per la complementazione. Si può inoltre dimostrare che
\begin{itemize}
	\item Se $P(A) \in \{ 0, 1 \}$ allora $A$ è indipendente da ogni altro evento.
	\item Se $A \cap B = \emptyset$ allora $A$ e $B$ non sono indipendenti, a meno che $P(A)$ o
	      $P(B)$ siano 0.
\end{itemize}

\begin{example}
	Si vuole estrarre una carta da un mazzo di 40 carte napoletane
	\[ \Omega = \{ \text{tutte le carte} \} \]
	e ci chiediamo se i due eventi
	\[ A = \{ \text{asso} \} \quad B = \{ \text{denari} \} \]
	sono indipendenti. La probabilità di estrarre un asso equivale a
	\[ P(A) = \frac{4}{40} = \frac{1}{10} \]
	La probabilità di estrarre una carta di denari è invece
	\[ P(B) = \frac{10}{40} = \frac{1}{4} \]
	La probabilità di estrarre l'asso di denari equivale a
	\[ P(A \cap B) = \frac{1}{40} = \frac{1}{10} \cdot \frac{1}{4} = P(A) P(B) \]
	Quindi gli eventi sono indipendenti.
\end{example}

\subsection{Indipendenza per 3 o più eventi}
Siano $A$, $B$ e $C$ degli eventi. Per riuscire a dire che sono indipendenti c'è bisogno che
\begin{itemize}
	\item Siano a due a due indipendenti
	      \begin{align*}
		      P(A \cap B) = & P(A) \cdot P(B) \\
		      P(A \cap C) = & P(A) \cdot P(C) \\
		      P(B \cap C) = & P(B) \cdot P(C)
	      \end{align*}
	\item La probabilità dell'intersezione di tutti e tre si spezzi come il prodotto delle
	      probabilità dei tre eventi.
	      \[ P(A \cap B \cap C) = P(A) \cdot P(B) \cdot P(C) \]
\end{itemize}

\begin{example}
	Consideriamo lo spazio campionario $\Omega=\{ 1,2,3,4 \}$ e i relativi sottoinsiemi
	\[ A=\{ 1, 2 \} \quad B=\{ 1, 3 \} \quad C=\{ 2, 3 \} \]
	con $P$ uniforme su $\Omega$. La probabilità che i singoli eventi si verifichino è di
	\[ P(A) = P(B) = P(C) = \frac{2}{4} = \frac{1}{2} \]
	La probabilità dell'intersezione degli eventi presi a due a due è di
	\[ P(A \cap B) = P(A \cap C) = P(B \cap C) = \frac{1}{4} \]
	Dunque la prima condizione per l'indipendenza è soddisfatta. Andiamo a calcolare ora quanto
	vale l'intersezione dei tre eventi. Consideriamo l'intersezione $A \cap (B \cap C)$. Dunque
	possiamo calcolare la probabilità dell'intersezione in questo modo
	\[ P(A \cap (B \cap C)) = P(A | (B \cap C)) = 0 \neq P(A) \]
	Dunque sapere che accade $B \cap C$ cambia la probabilità che accada $A$.
\end{example}

\begin{definition}
	Dati $A_1, A_2, \dots, A_n$ eventi, questi si dicono \textbf{indipendenti} se $\forall k$
	intero con $1 \leq k \leq n$ e $\forall \; 1 \leq i_1 < \dots < i_k \leq n$ vale
	\[ P(A_{i_1} \cap \dots \cap A_{i_k}) = P(A_{i_1}) \cdot \dots \cdot P(A_{i_k}) \]
\end{definition}

In altre parole la probabilità dell'intersezione degli eventi si deve poter spezzare come
prodotto delle probabilità dei singoli eventi.

\begin{observation}
	Se $A_1, A_2, \dots, A_n$ sono indipendenti allora sono indipendenti a due a due.
\end{observation}

\subsubsection{Prove ripetute e schema di Bernoulli}
Un caso importante di eventi indipendenti è il caso delle \textbf{prove ripetute} in cui si ripete
un esperimento $n$ volte nelle medesime condizioni. Possiamo dire che gli eventi riferiti a
ripetizioni distinte o a gruppi disgiunti di ripetizione sono indipendenti.

Un sottocaso importante delle prove ripetute è lo \textbf{schema di Bernoulli} per $n$ prove
ripetute in cui ciascuna prova ha esito \emph{successo} o \emph{insuccesso}. In questo specifico
caso è possibile scrivere lo spazio campionario come segue
\[ \Omega = \{ a_1, a_2, \dots, a_n | \forall i \; a_i \in \{ 0, 1 \} \} = \{ 0, 1 \}^n \]
dove associamo tipicamente a 1 il successo e a 0 l'insuccesso. La probabilità associata ad una
sequenza equivale a
\[ P(\{ a_1, \dots, a_n \}) = p^{\# \{i | a_i = 1\}} \cdot (1 - p)^{\# \{i | a_i = 0\}} \]
dove $p$ è la probabilità di successo della singola prova. A questo punto, se volessimo calcolare
la probabilità di ottenere $k$ successi dovremmo contare quanti possibili modi ci sono di ottenere
$k$ successi in $n$ prove ripetute. Per farlo possiamo pensare ad ogni prova come ad un evento e
quello che vogliamo fare è calcolare in quanti possibili modi è possibile estrarre $k$ successi da
un insieme di $n$ prove (coefficiente binomiale). Otteniamo quindi che
\[ \binom{n}{k} \cdot p^k \cdot (1 - p)^{n - k} \]
è la probabilità di ottenere esattamente $k$ successi in $n$ prove ripetute.

\begin{example}
	Lanciamo 10 volte una moneta. L'evento "testa" equivale al successo e l'evento "croce" equivale
	all'insuccesso. Qual è la probabilità di ottenere esattamente 3 volte testa? Prima di tutto
	abbiamo che
	\[ P(t) = P(c) = \frac{1}{2} \]
	quindi la probabilità che si verifichi una singola sequenza con 3 successi e 7 insuccessi si
	calcola come
	\[
		\left( \frac{1}{2} \right)^3 \cdot \left( 1 - \frac{1}{2} \right)^7
		= \left( \frac{1}{2} \right)^{10}
	\]
	Essendo interessati però a tutti i possibili modi in cui questo può avvenire e per farlo
	possiamo pensare di "estrarre" da un insieme di 10 lanci (prove), 3 successi e calcolare in
	quanti modi è possibile farlo. Otteniamo quindi
	\[
		\binom{10}{3} \cdot \left( \frac{1}{2} \right)^3 \cdot \left( 1 - \frac{1}{2} \right)^7
		= 120 \cdot \left( \frac{1}{2} \right)^{10} \approx 0.117
	\]
\end{example}

Se volessimo calcolare la probabilità di avere almeno $k$ successi per $n$ prove ripetute, dovremmo
calcolare la probabilità di ottenere da $k$ a $n$ successi e poi sommarle tutte. Discorso analago
per il problema complementare.

\begin{observation}
	Il concetto di indipendenza non è legato al concetto di causalità.
\end{observation}
