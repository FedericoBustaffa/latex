\section{Stima parametrica}
La stima parametrica si occupa di fornirci dei metodi per scegliere uno
\hyperref[def: stimatore]{stimatore} più buono possibile. Noi ne vedremo due:
\begin{itemize}
	\item Metodo di verosimiglianza
	\item Metodo dei momenti
\end{itemize}
i quali ci permetteranno di scegliere un buon stimatore per una certa statistica.

\subsection{Metodo di verosimiglianza}
Iniziamo con il considerare un campione statistico la cui legge di probabilità dipende da un
parametro $\theta \in \Theta$, nel quale le variabili possono essere discrete con funzione di
massa $p_\theta(x)$, oppure con densità $f_\theta(x)$.

\begin{definition}
	Si chiama \textbf{funzione di verosimiglianza} la funzione $L(\theta; \dots)$ definita, nel
	caso di variabili discrete, da
	\[ L(\theta, x_1, \ldots, x_n) = \prod_{i=1}^n p_\theta(x_i) \]
	e nel caso di variabili con densità, da
	\[ L(\theta, x_1, \ldots, x_n) = \prod_{i=1}^n f_\theta(x_i) \]
\end{definition}

Si noti che nel caso discreto la verosimiglianza equivale alla funzione di massa congiunta di
$(X_1, \dots, X_n)$. Analoga interpretazione nel caso di variabili con densità.

\begin{definition}
	Si chiama \textbf{stima di massima verosimiglianza}, se esiste, una statistica campionaria,
	usualmente indicata $\hat{\theta} = \hat{\theta} (x_1, \dots, x_n)$, tale che valga
	l'uguaglianza
	\[
		L(\hat{\theta}, x_1, \dots, x_n) = \max_{\theta \in \Theta} (L(\theta, x_1, \dots, x_n))
	\]
	per ogni $(x_1, \dots, x_n)$. Vogliamo cioè che $\hat{\theta}$ sia un punto di massimo per
	la funzione di verosimiglianza.
\end{definition}

Nel caso discreto, se $x_1, \dots, x_n$ sono gli esiti del campione, la stima di massima
verosimiglianza sceglie un parametro $\hat{\theta}$ che \emph{massimizza} la probabilità degli
esiti $x_1, \dots, x_n$.

\begin{example}
	Consideriamo un campione di variabili esponenziali $X \sim E(\theta)$ con $\theta > 0$ non noto
	e proviamo a stimare $\theta$ con il metodo di verosimiglianza, abbiamo che
	\[
		L(\theta, x_1, \dots, x_n) = \prod_{i=1}^n f_\theta(x_i) =
		\begin{cases}
			\theta e^{-\theta x_1} \cdots \theta e^{-\theta x_n} & x_i > 0 \quad \forall i       \\
			0                                                    & \text{se } \exists x_i \leq 0
		\end{cases}
	\]
	che può essere riscritta come
	\[
		L(\theta, x_1, \dots, x_n) =
		\prod_{i=1}^n f_\theta(x_i) =
		\begin{cases}
			\theta^n e^{-\theta \cdot \sum_{i=1}^{n} x_i} & x_i > 0 \quad \forall i       \\
			0                                             & \text{se } \exists x_i \leq 0
		\end{cases}
	\]
	Possiamo ora dimostrare che, data $g(\theta) = L(\theta, x_1, \dots, x_n)$, vale
	\[ \lim_{\theta \to 0} g(\theta) = 0 \quad \lim_{\theta \to +\infty} g(\theta) = 0 \]
	quindi $g$ ha almeno un massimo in $\hat{\theta}$ che verifica $g'(\hat{\theta}) = 0$
	\begin{align*}
		g'(\theta) = & n \cdot \theta^{n-1} \cdot e^{-\theta \cdot \sum_{i=1}^n x_i} +
		\theta^n \left( -\sum_{i=1}^{n} x_i \right) e^{-\theta \cdot \sum_{i=1}^{n} x_i} \\
		=            & \theta^{n-1} \cdot e^{-\theta \cdot \sum_{i=1}^{n} x_i} \cdot
		\left(n - \theta \cdot \sum_{i=1}^{n} x_i \right)
	\end{align*}
	Abbiamo quindi che $g'(\theta) = 0$ se e solo se
	\[
		n - \theta \cdot \sum_{i=1}^{n} x_i = 0 \iff
		\frac{1}{\theta} = \frac{1}{n} \cdot \sum_{i=1}^{n} x_i \iff
		\frac{1}{\theta} = \bar{x}
	\]
	Possiamo quindi concludere che $\hat{\theta} = 1 / \bar{x}$ è il parametro scelto.
\end{example}

\begin{example}
	Consideriamo un campione di variabili aleatorie uniformi $X \sim U((0, \theta))$, la cui
	densità è dunque
	\[
		f_\theta (x) = \begin{cases}
			\dfrac{1}{\theta} & x \in (0, \theta)    \\[2ex]
			0                 & x \notin (0, \theta)
		\end{cases}
	\]
	Proviamo ora a stimare $\theta$ con il metodo di verosimiglianza. Per prima cosa calcoliamo la
	funzione di verosimiglianza
	\[
		L(\theta, x_1, \dots, x_n) = \prod_{i=1}^n f_\theta(x_i) = \begin{cases}
			0                  & \text{se } \exists x_i \leq 0       \\
			\frac{1}{\theta^n} & x_i \in (0, \theta) \quad \forall i \\
			0                  & \text{se } \max_i (x_i) \geq \theta
		\end{cases}
	\]
	Dato che ogni $x_i$ ha la stessa probabilità degli altri esiti di essere ottenuto, deduciamo è
	ragionevole pensare che
	\[ \hat{\theta} = \max(x_1, \dots, x_n) \]
	poiché al di fuori di $\theta$ la probabilità è 0 e dunque non avremmo potuto ottenere alcun
	$x_i$.
\end{example}

\subsection{Metodo dei momenti}
Passiamo ora al metodo dei momenti, la cui idea è quella di confrontare i \emph{momenti teorici}
(valori attesi di $X^k$)
\[ m_k(\theta) = \E_\theta [X^k] \]
dove $\E_\theta$ equivale al valore atteso quando il parametro cercato è uguale a $\theta$ e
$k = 1,2,\dots$, con i momenti \emph{empirici} (cioè la media campionaria di $x_1^k, \dots, x_n^k$),
ossia
\[ \frac{1}{n} \sum_{i=1}^k x_i^k \]
Poiché la media campionaria è un buon stimatore del valore atteso, è ragionevole prendere, come
stima di $\theta$, un valore $\tilde{\theta}$ che realizzi l'uguaglianza tra alcuni momenti teorici
e i corrispondenti momenti empirici.

\begin{definition}
	Si chiama \textbf{stima con il metodo dei momenti}, se esiste, una statistica campionaria
	$\tilde{\theta} = \tilde{\theta}(x_1, \dots, x_n)$ tale che eguagli momenti teorici con i
	relativi momenti empirici
	\[ \E_{\tilde{\theta}} [X^k] = \frac{1}{n} \sum_{i=1}^{n} x_i^k \]
	per ogni $(x_1, \dots, x_n)$ e per uno o più interi positivi $k$.
\end{definition}

Le definizioni date sopra per il metodo di verosimiglianza e per il metodo dei momenti si
generalizzano in modo naturale al caso di più parametri, prendendo
$\theta = (\theta_1, \dots, \theta_m)$.

Non sempre queste stime esistono e quando esistono a volte danno valori simili e a volte no. Questo
perché il problema della stima di un parametro non è ben posto poiché non si può stimare
\emph{esattamente} il parametro.

Si può dimostrare che una stima per la coppia (valore atteso, varianza) con il metodo dei momenti
è data dalla coppia di parametri
\[ \begin{pmatrix} \bar{X}_n, & \dfrac{n-1}{n} \cdot S_n^2 \end{pmatrix} \]
per $k = 1, 2$.

\begin{example}
	Consideriamo un campione di variabili esponenziali $X \sim E(\theta)$ con $\theta > 0$ non noto
	e con densità
	\[
		f_\theta(x) = \begin{cases}
			\theta e^{-\theta x} & x > 0    \\
			0                    & x \leq 0
		\end{cases}
	\]
	e proviamo a stimare $\theta$ con il metodo dei momenti. Poniamo $k=1$ e, come sappiamo, il
	valore atteso della variabile esponenziale vale
	\[ \E[X] = \frac{1}{\theta} \]
	Per effettuare la stima con il metodo dei momenti dobbiamo imporre dunque
	\[ \frac{1}{\theta} = \frac{1}{n} \cdot \sum_{i=1}^n x_i \]
	da cui ricaviamo $\tilde{\theta} = 1 / \bar{x}$ come nel caso di stima di verosimiglianza.
\end{example}

\begin{example}
	Consideriamo un campione di variabili aleatorie uniformi $X \sim U((0, \theta))$, la cui
	densità è dunque
	\[
		f_\theta(x) = \begin{cases}
			\dfrac{1}{\theta} & x \in (0, \theta)    \\[2ex]
			0                 & x \notin (0, \theta)
		\end{cases}
	\]
	Proviamo a stimare $\theta$ con il metodo dei momenti studiando solo il primo momento ($k=1$).
	Imponiamo quindi
	\[
		\E_{\tilde{\theta}}[X] = \frac{1}{n} \cdot \sum_{i=1}^n \iff
		\frac{\tilde{\theta}}{2} = \frac{1}{n} \cdot \sum_{i=1}^n
	\]
	da cui ricaviamo $\tilde{\theta} = 2 \cdot \bar{x}_n$ diversamente da quanto ottenuto
	con il metodo di verosimiglianza.
\end{example}