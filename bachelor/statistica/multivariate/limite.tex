\section{Teoremi limite}
I \textbf{teoremi limite} entrano in gioco quando si vuole, in un certo senso, passare al lato
più empirico della statistica, ecco che vengono introdotti la \textbf{legge dei grandi numeri} e
il \textbf{teorema centrale del limite}.

Prendiamo per esempio il lancio di una moneta equilibrata. Se si effettuano 1000 lanci ci
aspettiamo di ottenere all'incirca 500 teste se la moneta è equilibrata. La legge dei grandi numeri
cerca di formalizzare questo risultato, mentre il teorema centrale del limite cerca di quantificare
le oscillazioni del numero di teste attorno a 500. Iniziamo però a dare qualche definizione.

\begin{definition}
	Data una famiglia di variabili aleatorie $X_1, X_2, \dots, X_n$ (possibilmente infinita), le
	variabili $X_i$ si dicono \textbf{indipendenti e identicamente distribuite} (i.i.d.) se sono
	indipendenti e hanno la stessa distribuzione (cioè $P_{X_i}$ non dipende da $i$).
\end{definition}

Nel caso di famiglia infinita, diciamo che le $X_i$ sono indipendenti se per ogni $n \in \N$ le
variabili $X_1, \dots, X_n$ sono indipendenti. Dire che le variabili $X_i$ sono i.i.d. equivale a
dire che le $X_i$ hanno la stessa funzione di ripartizione
\[ P(X_i \leq t) = F_{X_i} (t) = F(t) \]
per ogni $i$ e inoltre significa che sono indipendenti
\[ P(X_1 \leq t_1, \dots, X_n \leq t_n) = F_{X_1} (t_1) \cdots F_{X_n} (t_n) \]
per ogni $t_1, \dots, t_n \in \R$. Il tipico esempio di variabili aleatorie i.i.d è dato dalle
ripetizioni di un esperimento (non perforza con esisto successo o insuccesso).

Consideriamo $n$ (o anche infinite) ripetizioni di un esperimento nelle stesse condizioni. Sia
$X$ la variabile aleatoria che descrive un carattere dell'esperimento (ad esempio l'esito del
lancio di un dado). Per $i \in \N^+$, sia $X_i$ il valore del carattere dell'esito dell'$i$-esimo
esperimento, allora le $X_i$ sono i.i.d.

Come caso particolare consideriamo l'estrazione di un campione da una popolazione reale con $X$ la
variabile aleatoria che rappresenta un carattere degli individui. Supponiamo ora di estrarre
casualmente $n$ individui, in modo indipendente l'uno dall'altro, e chiamiamo $X_i$ il carattere
dell'$i$-esimo individuo estratto. Allora gli $X_i$ sono i.i.d con la stessa distribuzione di $X$.

Per comprendere meglio la situazione dobbiamo introdurre la \textbf{media campionaria} di $n$
variabili aleatorie i.i.d. $X_1, \dots, X_n$, la quale è definita come segue
\[ \bar{X} = \bar{X}_n = \frac{X_1 + X_2 + \dots + X_n}{n} \]
Osserviamo che $\bar{X}$ è essa stessa una variabile aleatoria in quanto combinazione di
variabili aleatorie.

\begin{example}
	Consideriamo il caso di $n$ ripetizioni di un esperimento la cui probabilità di successo è $p$
	e $X_i$ è la variabile aleatoria di Bernoulli associata all'$i$-esima ripetizione. Abbiamo
	quindi che le $X_i$ sono i.i.d e con distribuzione $B(p)$, quindi $X = X_1 + \dots + X_n$ è una
	variabile aleatoria $B(n,p)$ e rappresenta la \textbf{frequenza assoluta} del successo (di $A$)
	mentre $\bar{X} = \frac{X}{n}$ rappresenta la \textbf{frequenza relativa} del successo.
\end{example}

\subsection{Legge dei grandi numeri}
Uno degli obbiettivi fondamentali della \textbf{legge dei grandi numeri} è studiare il valore di
$\bar{X}_n$ per campioni grandi ($n$ grande). Questo significa calcolare in qualche modo il
limite, ma trattandosi di variabili aleatorie dobbiamo definire meglio cosa si intenda per limite.
Dobbiamo quindi definire il concetto di \textbf{convergenza in probabilità}.

\begin{definition}
	Diciamo che una successione $X_1, \dots, X_n, \dots$ di variabili aleatorie definite sullo
	stesso spazio di probabilità \textbf{converge in probabilità} a una variabile aleatoria $X$ se
	\[ P(|X_n - X| > \varepsilon) \to 0 \]
	per ogni $\varepsilon > 0$. In alternativa possiamo vedere la cosa in questo modo
	\[ \lim_{n \to +\infty} P(|X_n - X| > \varepsilon) = 0 \]
	Stiamo quindi dicendo che per $n$ grande $X_n$ è vicina a $X$ con probabilità alta.
\end{definition}

\begin{theorem}[Legge dei grandi numeri]\label{th: lgn}
	Sia $X_1, \dots, X_n, \dots$ una successione di variabili aleatorie i.i.d dotate di momento
	secondo finito e sia $\mu = \E[X_i]$, allora $\bar{X}_n$ converge in probabilità a $\mu$, cioè
	\[ P(|\bar{X}_n - \mu| > \varepsilon) \to 0 \]
	per $n \to +\infty$ e per ogni $\varepsilon > 0$.
	\begin{proof}
		L'idea alla base della dimostrazione è che si può dimostrare che
		\[
			\E[\bar{X}_n] = \E\left[ \frac{X_1 + \dots + X_n}{n} \right]
			=               \frac{1}{n} \sum_{i=1}^n \E[X_i] = \frac{1}{n} \sum_{i=1}^n \mu = \mu
		\]
		Calcoliamo ora la varianza di $\bar{X}_n$
		\begin{multline*}
			\Var(\bar{X}_n) =
			\Var \left( \frac{1}{n} \cdot \sum_{i=1}^n X_i \right) =
			\frac{1}{n^2} \cdot \Var \left( \sum_{i=1}^n X_i \right)  \\ =
			\frac{1}{n^2} \cdot \sum_{i=1}^n \Var(X_i) =
			\frac{1}{n^2} \cdot n \cdot \sigma^2 = \frac{\sigma^2}{n}
		\end{multline*}
		Per la disuguaglianza di Chebyshev abbiamo che
		\[
			P(|\bar{X}_n - \E[\bar{X}_n]| >
			\varepsilon) \leq \frac{\Var(\bar{X}_n)}{\varepsilon^2} =
			\frac{1}{\varepsilon^2} \cdot \frac{\sigma^2}{n} \to 0
		\]
		per $n \to \infty$.
	\end{proof}
\end{theorem}

In altre parole il teorema dice che la media campionaria di $n$ prove tende, per $n$ grande, al
valore atteso della singola prova.

\begin{corollary}\label{cor: lgn}
	Sia $X_1, \dots, X_n, \dots$ una successione di variabili aleatorie dotate di momento quarto
	finito e sia $\sigma^2$ la loro varianza, allora $S_n^2$ converge in probabilità a $\sigma^2$,
	cioè, per ogni $\varepsilon > 0$ vale
	\[ P(|S_n^2 - \sigma^2| > \varepsilon) \to 0 \]
	per $n \to \infty$.
\end{corollary}

\begin{example}
	Lanciamo un dado 1000 volte e vogliamo fornire un valore approssimato per il numero di 5 nei
	1000 lanci. In questo caso le $X_i \sim B(1/6)$ indipendenti e abbiamo che
	\[ \bar{X}_n = \frac{1}{n} \cdot \sum_{i=1}^{n} X_i \]
	è la frequenza relativa di 5. Per la LGN $\bar{X}_n \to 1/6$ e quindi se definiamo
	\[ Y_n = \sum_{i=1}^{n} X_i \]
	la frequenza assoluta di 5 abbiamo che che $Y_n \to n \cdot 1/6$. Per $n=1000$ abbiamo che
	\[ Y_{1000} \approx 1000 \cdot \frac{1}{6} \approx 167 \]
	Come possiamo notare, per $n$ grande, la frequenza relativa tende al probabilità teorica.
\end{example}

Introduciamo ora la \textbf{varianza campionaria} come la misura di quanto i campioni si
discostano dalla media la quale è definita come
\[ S^2 = S_n^2 = \frac{\sum_{i=1}^n \left( X_i - \bar{X}_n \right)^2}{n - 1} \]
Se $(X_1, \dots, X_n)$ rappresenta un campione, allora $S^2$ è la varianza campionaria di tale
campione.

\subsection{Teorema centrale del limite}
Come anticipato, questo strumento matematico ci fornisce una misura di quanto ci discostiamo dal
valore atteso della media.

Quello che ci dice il teorema centrale del limite è che le fluttuazioni intorno alla media
hanno un comportamento, per $n$ grande, universale e gaussiano a prescindere dall'esperimento in
esame.

\begin{definition}
	Sia $Y_1, \dots, Y_n, \dots$ una successione di variabili aleatorie, $Y$ una variabile
	aleatoria e siano $F_n$ ed $F$ le funzioni di ripartizione rispettivamente di $Y_n$ e $Y$.
	Supponendo che $F$ sia continua, diciamo che $Y_n$ \textbf{converge in legge} (o in
	distribuzione) a $Y$ se
	\[ \lim_{n \to +\infty} F_n (t) = F(t) \]
	per ogni $t \in \R$. Notiamo che questa condizione di convergenza dipende solo dalle leggi di
	$Y_n$ e di $Y$.
\end{definition}

\begin{theorem}[Teorema centrale del limite]\label{th: tcl}
	Sia $X_1, \dots, X_n, \dots$ una successione di variabili aleatorie i.i.d dotate di momento
	secondo finito, allora $\mu = \E[X_i]$ e $\sigma^2 = \Var(X_i) < +\infty$, allora
	\[ \sqrt{n} \cdot \frac{\bar{X}_n - \mu}{\sigma} \]
	converge in legge a $Z \sim N(0,1)$. Questo vuol dire che per ogni $a, b$ tali che
	$-\infty \leq a < b \leq +\infty$
	\begin{align*}
		\lim_{n \to +\infty} P \left( a \leq \sqrt{n} \cdot \frac{\bar{X}_n - \mu}{\sigma}
		\leq b \right) = & P(a \leq Z \leq b)                                      \\
		=                & \int_{a}^{b} \frac{1}{\sqrt{2 \pi}} \cdot e^{-x^2/2} dx \\
		=                & \Phi(b) - \Phi(a)
	\end{align*}
\end{theorem}

\begin{observation}
	Osserviamo che
	\[
		\sqrt{n} \cdot \frac{\bar{X}_n - \mu}{\sigma} =
		\frac{X_1 + \dots + X_n - n \cdot \mu}{\sigma \cdot \sqrt{n}}
	\]
	Vale anche che
	\[
		\E \left[ \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} \right] = 0 \quad
		\Var \left( \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} \right) = 1
	\]
\end{observation}

\begin{observation}
	La legge di $X_i$ può essere qualunque a patto che abbia momento secondo.
\end{observation}

Come \textbf{regola empirica} per capire quando $n$ è abbastanza grande perché il teorema dia
una buona approssimazione, possiamo considerare $n \geq 50$ per una buona approssimazione e
$n \geq 80$ per un'ottima approssimazione.

Un caso importante è quello in cui abbiamo $n$ ripetizioni di un esperimento di Bernoulli in cui
consideriamo $n$ variabili aleatorie con esito successo o insuccesso. Sappiamo che la variabile
aleatoria
\[ Y_n = X_1 + \dots + X_n \sim B(n, p) \]
rappresenta la frequenza assoluta del successo nelle prime $n$ ripetizioni. Il teorema centrale del
limite ci dice che
\[
	\sqrt{n} \cdot \frac{\bar{X}_n - p}{\sqrt{p \cdot (1-p)}} =
	\frac{Y_n - n \cdot p}{\sqrt{n \cdot p \cdot (1-p)}}
\]
è approssimativamente $N(0,1)$. Questo risultato prende il nome di
\textbf{approssimazione della binomiale} per $n$ grande. In particolare, come regola empirica,
possiamo prendere $n$ tale che
\[
	n \cdot p \cdot (1 - p) \geq \begin{cases}
		15 & \text{per una buona approssimazione} \\
		20 & \text{per un'ottima approssimazione}
	\end{cases}
\]

\begin{example}
	Consideriamo 1000 lanci di moneta e ci chiediamo
	\begin{enumerate}
		\item Qual è la probabilità di avere almeno 480 teste.
		\item Quanto deve valere $k$ tale che, con probabilità del 95\%, escano almeno $k$ teste su
		      1000.
	\end{enumerate}
	Sia $X$ il numero di teste su 1000 lanci $\sim B(1000, 1/2)$. Per poter applicare il tcl
	dobbiamo verificare che
	\[ n \cdot p \cdot (1-p) = 1000 \cdot \frac{1}{2} \cdot \frac{1}{2} = 250 \geq 20 \]
	Quindi
	\begin{align*}
		P(X \geq 480) = & P \left( \frac{X - 1000 \cdot \frac{1}{2}}
		{\sqrt{1000 \cdot \frac{1}{2} \cdot \frac{1}{2}}} \geq
		\frac{480 - 1000 \cdot \frac{1}{2}}{\sqrt{1000 \cdot \frac{1}{2} \cdot \frac{1}{2}}}
		\right)                                                                  \\
		=               & P \left( \frac{X - 500}{\sqrt{250}} \geq -1.26 \right) \\
	\end{align*}
	Se adesso introduciamo $Z \sim N(0,1)$, possiamo dire, per il tcl, che
	\[
		P \left( \frac{X - 500}{\sqrt{250}}
		\geq -1.26 \right) \approx P(Z \geq -1.26)
	\]
	da cui otteniamo
	\begin{align*}
		P(Z \geq -1.26) = & 1 - P(Z < -1.26)         \\
		=                 & 1 - \Phi (-1.26)         \\
		=                 & \Phi(1.26) \approx 0.896
	\end{align*}
	Vogliamo ora trovare $k$ tale che $P(X \geq k) = 0.95$ che equivale a
	\[
		P(X \geq k) = P \left( \frac{X - 500}{\sqrt{250}} \geq \frac{k - 500}{\sqrt{250}} \right)
	\]
	Sempre per il tcl abbiamo che
	\[
		P \left( \frac{X - 500}{\sqrt{250}} \geq
		\frac{k - 500}{\sqrt{250}} \right) \approx
		P\left( Z \geq \frac{k - 500}{\sqrt{250}} \right) = 0.95
	\]
	Come possiamo vedere il problema è quello di trovare un quantile, ossia un valore $k$ tale che
	\[
		P \left( Z \geq \frac{k - 500}{\sqrt{250}} \right) = 0.95 \iff
		1 - P \left( Z < \frac{k - 500}{\sqrt{250}} \right) = 0.95
	\]
	e quindi
	\[
		1 - \Phi \left( \frac{k - 500}{\sqrt{250}} \right) = 0.95 \iff
		\Phi \left( \frac{k - 500}{\sqrt{250}} \right) = 0.05
	\]
	Tramite le tavole otteniamo che, affinché l'ultima uguaglianza sia vera, deve valere
	\[ \frac{k - 500}{\sqrt{250}} = -1.64 \iff k \approx 474.07 \]
	Concludiamo quindi che con probabilità del 95\% otterremo almeno 474 teste.
\end{example}

\begin{proposition}\label{prop: tcl}
	Sia $X_1, \dots, X_n, \dots$ una successione di variabili aleatorie i.i.d con momento secondo
	finito, $\mu = \E[X_i]$ e $\sigma^2 = \Var(X_i)$ supponendo $\sigma^2 > 0$, allora
	\[ \sqrt{n} \cdot \frac{\bar{X}_n  - \mu}{S_n} \]
	tende in legge a $Z \sim N(0,1)$ per $n \to \infty$. Possiamo quindi dire che per ogni $a,b$
	tali che $-\infty < a < b < +\infty$ vale
	\[
		P \left( a \leq \sqrt{n} \cdot \frac{\bar{X}_n  - \mu}{S_n} \leq b \right) \to
		P(a \leq Z \leq b)
	\]
	dove
	\[ S_n^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X}_n)^2 \]
\end{proposition}