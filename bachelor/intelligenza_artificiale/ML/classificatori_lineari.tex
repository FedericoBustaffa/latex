\chapter{Classificatori lineari}
Fino ad ora abbiamo utilizzato i modelli lineari solo per trattare problemi di regressione. In realt\`a i modelli
lineari sono molto utili anche per affrontare problemi di classificazione.

Se prima il modello lineare usato per la regressione restituiva un valore numerico reale, adesso si limiter\`a a
restituire un numero finito di valori discreti (tanti quante sono le possibile classificazioni di un dato).

\section{Classificatore lineare binario}
Per capire meglio il problema ci limitiamo al caso di un problema di \textbf{classificazione binario}, ossia un con
due soli valori possibili in output (in genere \verb|true| e \verb|false|).

Dato che il nostro modello si muove comunque nello spazio continuo numerico dobbiamo avere un valore per indicare
\verb|false| (in genere $0$ o $-1$) e uno per indicare \verb|true| (in genere $1$).

Si procede definendo una zona dell'iperpiano $wx$ come \emph{negativa} e una come \emph{positiva}. Se il dato si
trova nella zona negativa dell'iperpiano negativa il valore di ritorno sar\`a \verb|false|, se si trova in quella
positiva sar\`a \verb|true|.

Per trovare l'approssimazione utilizziamo sempre l'algoritmo (LMS) con la solita notazione. Ci\`o che cambia (ma
che all'algoritmo non interessa) \`e che ora gli unici valori target nel training set saranno $-1/0$ e $1$.

\begin{definition}
	Chiamo \textbf{decision boundary} il luogo geometrico dei punti tali che $x^T w = 0$. Al di fuori di esso ci sono
	le zone definite negative e positive dell'iperpiano.
\end{definition}
La funzione che useremo per la fase di predizione sar\`a quindi
\[
	h(x) = \begin{cases}
		1 & \text{se } x^T w \geq 0 \\
		0 & \text{altrimenti}
	\end{cases}
\]
nel caso considerassimo $0$ e $1$ come valori target. Sar\`a invece
\[ h(x) = \text{sign}(x^T w) \]
nel caso considerassimo $-1$ e $1$ come valori target.

C'\`e da chiarire che l'algoritmo non cerca di minimizzare l'errore di classificazione, esso tratta infatti il problema
come un problema di regressione e minimizza l'errore in relazione alla posizione dei punti nell'iperspazio.

Precisiamo che la funzione da minimizzare \`e
\[ E(w) = \sum_{p = 1}^l (y_p - x^T w) \]
Non possiamo usare la notazione $h(x)$ come per la regressione perch\'e in questo caso andremmo ad usare
\[ h(x) = sign(x^T w) \]
mentre noi, come per la regressione, vogliamo agire sui pesi $w$.

Per il resto, tutto ci\`o che abbiamo detto per i problemi di regressione pu\`o essere usato per risolvere problemi di
classificazione con modelli lineari.

Possiamo quindi trattare il problema come un problema di regressione lineare, polinomiale e cos\`i via, andando anche
regolarizzarlo se necessario.

\subsection{Classificatore lineare univariato}
Il training set, come anticipato, contiene coppie di questo tipo:
\[ (x, y) \]
dove $x$ \`e l'unica variabile in input e $y$ \`e il valore target che assume valore $-1/0$, se negativo oppure $1$ se
positivo.

Noi vogliamo trovare una retta che ci fornisca una buona classificazione degli esempi di training.

Applichiamo quindi l'algoritmo e troveremo la retta
\[ x^T w = w_1 x + w_0 \]
Ora possiamo definire il decision boundary
\[ x^T w = 0 \]
il quale ci dice il punto in cui la retta interseca l'asse $x$. Per classificare nuovi dati in input non ci rimane che
vedere dove la retta \`e positiva e dove \`e negativa tramite la funzione
\[ h(x) = sign(x^T w) \]

\section{Regole congiuntive}
I modelli lineari possono anche essere usati per rappresentare regole congiuntive.

Infatti \`e possibile rappresentare un \emph{and} tra variabili booleane in questo modo.
\[ x_1 \wedge x_2 \]
Questo si converte in una disequazione del tipo:
\[ 1 \cdot x_1 + 1 \cdot x_2 > 2 \]
oppure
\[ 1 \cdot x_1 + 1 \cdot x_2 \geq 2.5 \]

Tramite questa disequazione divido in due parti l'iperpiano: una positiva (dove la disequazione \`e soddisfatta) e una
negativa.

Come si pu\`o osservare inserendo a caso in 0 o 1 in $x_1$ e $x_2$, la disequazione sar\`a soddisfatta solo quando la
catena di \emph{and} sar\`a soddisfatta e viceversa.

Ovviamente il discorso vale anche per input $n$-dimensionali.

\section{Problemi linearmente separabili}
Come gi\`a detto, l'algoritmo non minimizza l'errore di classificazione neanche quando \`e possibile dividere in modo
netto gli esempi negativi da quelli positivi.
\begin{definition}
	Due insiemi di punti in uno spazio $n$-dimensionale sono \textbf{linearmente separabili} se esiste un
	iperpiano $(n-1)$-dimensionale che li separa.
\end{definition}

Se usiamo LMS, il fatto che il problema sia linearmente separabile non ci d\`a comunque alcuna garanzia che questo trovi
una funzione in grado di classificare perfettamente i due insiemi. Questo vale non solo per funzioni lineari ma per
qualsiasi tipo di funzione.

Se questo \`e vero per problemi linearmente separabili lo \`e anche per problemi \textbf{non linearmente separabili}, sui
quali, LMS produrrebbe comunque errori di classificazione (anche aumentando di molto la complessit\`a del modello).