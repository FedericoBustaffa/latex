\chapter{Apprendimento non supervisionato}
In questo capitolo andremo a trattare l'\textbf{apprendimento non supervisionato}, nel quale, non vengono forniti gli
esempi di training nella forma che siamo abituati a vedere, ossia
\[ d = (x, y) \]
dove $x$ \`e l'input e $y$ \`e il relativo output. Nell'apprendimento non supervisionato abbiamo solo il valore $x$ e
quel che vogliamo fare \`e cercare dei raggruppamenti di questi dati.

\section{Clustering}
Una delle operazioni pi\`u comuni in questo tipo di apprendimento \`e quella del \textbf{clustering}, in cui andiamo
a ripartire i dati in gruppi (o \textbf{cluster}). Un cluster ha la particolarit\`a di avere al suo interno dati
\emph{simili}, ossia dati vicini fra loro, dove la distanza \`e calcolata tramite la formula della distanza Euclidea,
vista per il K-NN.

In genere si ha anche l'obbiettivo di trovare i cosiddetti \textbf{centroidi} o \textbf{prototipi}, ossia dei punti
nell'iperpiano che stanno al centro del cluster, quindi in un \emph{punto medio}, che sia in grado di rappresentare il
cluster.

Questo ci permette di scoprire strutture naturali e latenti dei dati in maniera automatica e ci permettere di scoprire
pattern sconociuti.

Un altro motivo \`e il \textbf{preprocessing} per altri metodi di Machine Learning: possiamo andare a ridurre la
dimensionalit\`a dei dati.

Possiamo anche affermare che pattern appartenenti allo stesso cluster, sono pi\`u simili fra loro che a pattern
appartenenti ad altri cluster.

\section{Spazio delle ipotesi}
In questo caso, lo spazio delle ipotesi per problemi di clustering, \`e un insieme di \textbf{vettori quantizzatori}
che associano un dato $x$ ad un centroide $c(x)$ di un certo cluster. Passiamo quindi da uno spazio \emph{continuo}
a uno \emph{discreto}.

L'obbiettivo \`e trovare il partizionamento ottimale per distribuzioni di dati sconociute in cluster approssimando
la computazione ad un centroide.

Una delle funzioni comuni per il calcolo dell'errore \`e questa
\[ L(h(x_p)) = \| x_p - c(x_p) \|^2 \]
la quale indica la distanza tra $x_p$ e il suo centroide ed \`e detta \textbf{funzione di errore di distorsione quadratico}.
Il nostro obbiettivo \`e \emph{minimizzare} questa funzione in modo da spostare il centroide al centro del cluster.

\section{K-means}
Il \textbf{K-means} \`e un algoritmo molto semplice e sfrutta l'errore quadratico per riuscire a minimizzare la funzione
vista in precedenza.
\begin{enumerate}
	\item Si sceglie il numero $k$ dei centroidi, e si assegna ognuno di essi a $k$ pattern scelti a caso o a $k$ punti
	      scelti a caso all'interno dell'ipervolume in cui stanno tutti i nostri pattern.
	\item Si assegna ogni pattern al centroide pi\`u vicino, ossia al centroide $i$ che minimizza
	      \[ \| x - c_i \|^2 \]
	      andiamo a creare in questo modo $k$ cluster.
	\item Si ricalcola la posizione di ogni centroide sulla base del cluster creato. Per farlo facciamo la media del
	      valore di tutti i pattern $x_j$ appartenenti a $cluster_i$:
	      \[ c_i = \frac{1}{|cluster_i|} \sum_{j : x_j \in cluster_i} x_j \]
	\item Una volta spostati i centroidi dobbiamo controllare che ogni pattern sia assegnato correttamente al centroide
	      pi\`u vicino. Se cos\`i non fosse si torna la punto 2. altrimenti si termina.
\end{enumerate}

\subsection*{Limitazioni}
L'algoritmo \`e semplice ma limitato:
\begin{itemize}
	\item Il numero di cluster da trovare \`e fissato a priori.
	\item La minimizzazione locale dell'errore rende l'algoritmo troppo dipendente dall'inizializzazione.
	\item Pu\`o funzionare bene per cluster compatti e \emph{ipersferici}.
	\item Non ha propriet\`a di visualizzazione poich\'e non \`e possibile proiettare il cluster in uno spazio con una
	      dimensionalit\`a pi\`u bassa.
\end{itemize}

\section{Valutazione clustering}
La valutazione di un metodo di clustering pu\`o essere \emph{soggettiva}, ma solo in particolari subdomini ben definiti
sui quali possiamo fare assunzioni di qualche tipo e aspettarci un certo output.

Un metodo di valutazione pi\`u \emph{oggettivo} pu\`o essere dato dall'errore di quantizzazione, che tuttavia dipende dal
numero di centroidi e dal loro posizionamento iniziale (due fattori scelti a priori dall'utente).

Per problemi di classificazione si potrebbe valutare il cluster in base al valore di \textbf{purezza}, ossia, si contano
gli esempi positivi e quelli negativi. Se il numero di positivi \`e nettamente superiore a quello dei negativi o viceversa,
diremo che il cluster \`e \emph{puro}, al contrario, pi\`u la proporzione tra le due classi si avvicina al 50\% pi\`u
il cluster sar\`a \emph{impuro}.

Il problema di questo metodo di valutazione sta nel fatto che noi abbiamo raggruppato i pattern in base alla vicinanza
ai centroidi e non in base al loro valore target. Abbiamo quindi un metodo di valutazione che non centra niente col
metodo di clustering usato.

Quindi non \`e corretto valutare il cluster in questo modo ma si pu\`o comunque fare per ricavare relazioni utili ad
analizzare il problema.

\section{Riduzione della dimensionalit\`a}
Nell'ambito dell'apprendimento non supervisionato possiamo effettuare una riduzione della dimensionalit\`a dei dati.

Nello specifico, vogliamo compiere un'operazione di questo tipo
\[ \langle x_1, x_2, \dots, x_n \rangle \rightarrow \langle x_1', x_2', \dots, x_{n'}' \rangle \]
dove $n > n'$ e dove i nuovi dati $x_i'$ sono combinazioni lineari dei precedenti $x_i$.

Per farlo abbiamo bisogno di una componente di calcolo chiamata \textbf{Principal Component Analisys} o \textbf{PCA},
la quale, calcola i nuovi assi lungo la direzione di massima varianza rispetto alla distribuzione originale dei dati.

Ora abbiamo un metodo per la riduzione della dimensionalit\`a che pu\`o essere usato anche per metodi supervisionati
che andavano incontro al problema della dimensionalit\`a quando questa diventava troppo alta.