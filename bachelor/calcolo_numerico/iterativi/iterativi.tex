\chapter{Metodi iterativi}
I metodi \textbf{iterativi} cercano di generare delle \textbf{successioni} che convergono nella soluzione
del sistema lineare. Questi metodi sono più interessanti per matrici sparse di grosse dimensioni, per le quali,
un metodo diretto come l'eliminazione gaussiana, risulta molto inefficiente sia in termini di complessità
computazionale sia in termini di spazio occupato in memoria. I metodi iterativi riescono invece a preservare la
\emph{sparsità} della matrice.

Supponiamo ora di voler trovare una soluzione per il sistema $Ax = b$ e supponiamo di poter decomporre la matrice
$A$ come differenza di due matrici.
\[ A = M - N \]
con $M$ invertibile ($\det(M) \neq 0$) allora vale
\begin{gather*}
	Ax = b \\
	\Leftrightarrow \\
	(M - N) x = b \\
	\Leftrightarrow \\
	M x = N x + b \\
	\Leftrightarrow \\
	x = M^{-1} N x + M^{-1} b
\end{gather*}
Osserviamo quindi che trovare la soluzione del sistema lineare $Ax = b$ equivale a trovare il vettore $x$ tale che
\[ x = Px + q \]
dove $P = M^{-1} N$ e $q = M^{-1} b$. Per generare la successione possiamo scegliere un vettore di partenza
$x^{(0)}$ e calcolare iterativamente gli altri vettori $x^{(k+1)}$.
\[
	\begin{cases}
		x^{(0)} \in \R^n \\
		x^{(k+1)} = P x^{(k)} + q
	\end{cases}
\]
Ovviamente la relazione $x^{(k+1)} = P x^{(k)} + q$ è esprimibile anche in forma estesa
\begin{gather*}
	x^{(k+1)} = P x^{(k)} + q \\
	\Leftrightarrow         \\
	M x^{(k+1)} = N x^{(k)} + b
\end{gather*}
Quest'ultima relazione è quella che viene di fatto implementata nei vari algoritmi. La matrice $P$ non viene quasi
mai esplicitamente generata perché si potrebbe perdere la sparsità della matrice.

Per riuscire a capire se una sequenza di vettori converge in un vettore $x$ deve valere la seguente relazione
\[ \lim_{k \to +\infty} x^{(k)} = x \Leftrightarrow \lim_{k \to +\infty} \norm{x^{(k)} - x} = 0 \]
Questa relazione vale a prescindere dalla norma che stiamo considerando. Se però consideriamo la norma infinito
vale che
\[ \norm{x^{(k)} - x}_\infty = \max | (x^{(k)} - x)_i | \quad i = 1, \dots, n \]
quindi vale
\[ 0 \leq | (x^{(k)} - x)_i | \leq \max | (x^{(k)} - x)_i | \quad i = 1, \dots, n \]
possiamo quindi affermare che, se consideriamo la norma infinito, la successione di vettori converge nel vettore
soluzione se, per ogni vettore della successione, ogni componente converge nella componente corrispondente del
vettore soluzione.

\begin{theorem}
	Se la successione dei vettori generati iterativamente converge allora converge nella soluzione che stiamo
	cercando
	\[ \lim_{k \to +\infty} x^{(k)} = x \in \R^n \Rightarrow x = Px + q \]
	\begin{proof}
		Dimostrare il teorema è semplice in quanto se
		\[ x^{(k+1)} = P x^{(k)} + q \]
		basta fare il limite per $k \to +\infty$ per ambo i membri e otteniamo
		\begin{align*}
			\lim_{x \to +\infty} x^{(k+1)} & = \lim_{k \to +\infty} P x^{(k)} + q \\
			                               & \Downarrow                           \\
			x                              & = P x + q
		\end{align*}
	\end{proof}
\end{theorem}

\section{Metodo di Jacobi}
Una possibile implementazione di metodo iterativo è il \textbf{metodo di Jacobi}. Questo prevede l'utilizzo
di una matrice $M$ definita come la matrice diagonale composta dagli elementi diagonali di $A$.
\[ M = \diag (A) \]
Ne segue che la matrice $N$ avrà la seguente forma
\[
	N = M - A = \begin{bmatrix}
		         & -a_{1,2} & \dots      & -a_{1,n}   \\
		-a_{2,1} &          & \ddots     & \vdots     \\
		\vdots   & \ddots   &            & -a_{n-1,n} \\
		-a_{n,1} & \dots    & -a_{n,n-1} &
	\end{bmatrix}
\]
Dato che per applicare il metodo descritto in precedenza la matrice $M$ deve essere invertibile, il meotodo di
Jacobi è realizzabile solo quando gli elementi sulla diagonale principale di $A$ sono tutti non nulli.

Vediamo ora quanto costa un'iterazione per il metodo di Jacobi. All'iterazione $k+1$ avremo che
\[ M x^{(k+1)} = N x^{(k)} + b \]
Considerando la generica riga $i$ dell'iterazione abbiamo che
\[ a_{i,i} x^{(k+1)}_i = b_i - \sum_{j=1, j \neq i}^n a_{i,j} x_j^{(k)} \quad i= 1, \dots, n \]
Otteniamo quindi l'$i$-esima componente del vettore $x$ all'iterazione $k+1$ calcolando
\[
	x^{(k+1)}_i =
	\frac{1}{a_{i,i}} \left( b_i - \sum_{j=1, j \neq i}^{n} a_{i,j} x_j^{(k)} \right) \quad i=1,\dots, n
\]
Ad ogni passo vengono compiuti tanti prodotti quanti sono gli elementi non nulli sulla riga $i$-esima. Se
indichiamo con $\nnz(A)$ gli elementi non nulli di $A$ allora il costo è di $\nnz(A)$ operazioni per passo.

Se la matrice è sparsa e riusciamo a convergere abbastanza velocemente allora facciamo all'incirca $n$ operazioni
per passo. Se la matrice è piena, al caso pessimo ne facciamo $n^2$.

\section{Metodo di Gauss-Seidel}
Questo metodo nasce come modifica del metodo di Jacobi poiché, in tale metodo, quando calcoliamo $x^{(k+1)}_i$,
abbiamo già calcolato le componenti del vettore $x^{(k+1)}$ dalla prima alla $(i-1)$-esima. Inoltre prendiamo
la matrice $M$ come la parte triangolare inferiore di $A$.

Esso sfrutta le componenti già calcolate per calcolare le successive. Per ottenere quindi la componente
$i$-esima del vettore $x^{(k+1)}$ calcoliamo
\[
	x^{(k+1)}_i =
	\frac{1}{a_{i,i}} \left( b_i - \sum_{j=1}^{i-1} a_{i,j} x^{(k)}_j -
	\sum_{j=i+1}^{n} a_{i,j} x^{(k)}_j \right)
\]
Nella prima sommatoria gli elementi non sono conosciuti solo al tempo $k$ ma anche al tempo $k+1$ e quindi la
formula sopra si modifica come segue
\[
	x^{(k+1)}_i =
	\frac{1}{a_{i,i}} \left( b_i - \sum_{j=1}^{i-1} a_{i,j} x^{(k+1)}_j -
	\sum_{j=i+1}^{n} a_{i,j} x^{(k)}_j \right)
\]
L'iterazione per questo metodo ha lo stesso costo di prima in quanto si fa un prodotto ogni volta che si ha un
elemento di $A$ non nullo. Abbiamo quindi un costo $O(\nnz(A))$ che equivale al costo del prodotto matrice per
vettore.
