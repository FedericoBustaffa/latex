\chapter{Algebra lineare e condizionamento}
In questo capitolo verranno trattati i problemi dell'algebra lineare e il loro condizionamento tra cui la
risoluzione di sistemi lineari e il calcolo di autovalori e autovettori di matrici.

Il primo problema che andiamo a trattare è quello della risoluzione di sistemi lineari. Dall'algebra lineare
noi sappiamo che un sistema lineare è rappresentabile come
\[ A x = b \]
con $A \in \R^{n \times n}$ e $b \in \R^n$. La soluzione del sistema è quindi il vettore $x \in \R^n$ tale che
\[ x = A^{-1} b \]
Come sappiamo è possibile scrivere ognuna delle componenti $x_i$ di $x$ come funzione $f_i$ di $A$ e $b$ in
questo modo
\[ x_i = f_i (a_{11}, \dots, a_{nn}, b_1, \dots, b_n) \quad i = 1 \dots n \]
Abbiamo quindi $n$ funzioni del tipo $f : \R^{n \times n} \times \R^n \to \R$. Questa rappresentazione ci
fornisce un modo per scrivere la soluzione del problema dal punto di vista matematico ma non ci dà alcuna
informazione sul condizionamento del problema.

In macchina non abbiamo $A$ e $b$ ma $\hat{A}$ e $\hat{b}$, rispettivamente \emph{vicini} ad $A$ e $b$. Il
concetto di \emph{vicinanza} tra due numeri $x$ e $y$ per esempio è esprimibile come $|x - y|$. Quanto più
questo valore sarà vicino a 0 tanto più i due numeri saranno per l'appunto \emph{vicini}.

Il valore assoluto possiede delle proprietà interessanti che ci serviranno per quello che andremo a trattare
tra poco. Prendiamo $f(x) = |x|$, vale che
\begin{itemize}
	\item $f(x) \geq 0$ e $f(x) = 0 \Leftrightarrow x = 0$
	\item $f(\alpha x) = \alpha \cdot f(x)$
	\item $f(x + y) \leq f(x) + f(y)$
\end{itemize}

\section{Norme vettoriali e matriciali}
Dopo la breve premessa introduciamo il concetto di \textbf{norma} e chiariamo meglio la sua utilità in questo
ambito.

\subsection{Norma vettoriale}
\begin{definition}
	Definiamo \textbf{norma vettoriale} su $\R^n$ di un vettore $v$, una funzione del tipo $f : \R^n \to \R$ che
	soddisfa le seguenti proprietà:
	\begin{itemize}
		\item $\forall v \in \R^n$ vale che
		      \[ f(v) > 0, \quad f(v) = 0 \Leftrightarrow v = 0 \]
		\item $\forall v \in \R^n$ e $\forall \alpha \in \R$ vale che
		      \[ f(\alpha v) = |\alpha| \cdot f(v) \]
		\item $\forall v, w \in \R^n$ vale che
		      \[ f(v + w) \leq f(v) + f(w) \]
	\end{itemize}
\end{definition}

Per comodoità di notazione indicheremo la norma vettoriale $f$ per il vettore $v$ con $f(v) = \| v \|$. Definiamo
inoltre la \textbf{distanza} tra $v$ e $w$ come la norma vettoriale della loro differenza:
\[ \dist(v, w) = f(v - w) = \| v - w \| \]
Una funzione norma tra le più comuni nell'algebra lineare è quella per il calcolo della lunghezza di un vettore
\[ f(v) = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2} = \sqrt{v^T v} = \| v \|_2 \]
Il 2 indica il fatto che stiamo facendo riferimento alla \textbf{norma 2} o \textbf{norma euclidea} del vettore $v$.
Tra le altre funzioni norma che utilizzeremo più spesso abbiamo la \textbf{norma 1} e la \textbf{norma infinito}:
\begin{itemize}
	\item $f(v) = |v_1| + |v_2| + \dots + |v_n| = \| v \|_1$
	\item $f(v) = \max(|v_i|) = \| v \|_\infty$ con $i = 1 \dots n$
\end{itemize}
In generale questi tre metodi di calcolo della norma di un vettore non forniscono lo stesso risultato. Quello che
conta è che qualsiasi scegliamo, un'analisi qualitativa fornisce gli stessi risultati.

\subsection{Norma matriciale}
Discorso analogo per il calcolo della distanza tra matrici.

\begin{definition}
	Definiamo \textbf{norma matriciale} su $\R^{n \times n}$ di una matrice $A$, una funzione del tipo
	$f : \R^{n \times n} \to \R$ che soddisfa le seguenti proprietà:
	\begin{itemize}
		\item $\forall A \in \R^{n \times n}$ vale che
		      \[ f(A) \geq 0, \quad f(A) = 0 \Leftrightarrow A = 0 \]
		\item $\forall A \in \R^{n \times n}$ e $\forall \alpha \in \R$ vale che
		      \[ f(\alpha \cdot A) = |\alpha| \cdot f(A) \]
		\item $\forall A, B \in \R^{n \times n}$ vale che
		      \[ f(A + B) \leq f(A) + f(B) \]
		\item $\forall A, B \in \R^{n \times n}$ vale che
		      \[ f(A \cdot B) \leq f(A) \cdot f(B) \]
	\end{itemize}
\end{definition}

Abbiamo aggiunto una proprietà in più ma il punto del discorso non cambia. Anche in questo caso se $f$ è una
norma matriciale su $\R^{n \times n}$ allora possiamo indicarla come $f(A) = \| A \|$ e possiamo definire la
distanza tra due matrici $A$ e $B$ come la norma della loro differenza:
\[ \dist(A, B) = f(A - B) = \| A - B \| \]
Per poter proseguire dobbiamo riuscire a definire delle norme matriciali legate in qualche modo alla norma
vettoriale.

\subsection{Norme matriciali indotte dalla norma vettoriale}
Introduciamo ora l'ultimo strumento necessario per poter risolvere e studiare problemi di algebra lineare in
macchina.
\begin{definition}
	Data una norma vettoriale su $\R^n$ si dice \textbf{norma matriciale indotta} la funzione
	$f : \R^{n \times n} \to \R$ tale che $\forall A \in \R^{n \times n}$ vale che
	\[ f(A) = \max \| A v \| \]
	sull'insieme $\{ v \in \R^n \mid \| v \| = 1 \}$, ossia dei vettori $v$ che hanno norma 1.
\end{definition}

Prendere una norma matriciale indotta ci permette di introdurre una proprietà fondamentale per l'analisi:
\[ \| A v \| \leq \| A \| \cdot \| v \| \]
Questo ci permette di maggiorare la lunghezza di $A v$ con la "\emph{lunghezza}" di $A$ per la lunghezza di $v$.

\subsubsection{Caratterizzazione}
L'insieme di vettori con norma 1 è infinito e per riuscire a trovare il massimo di $\| A v \|$ ricorriamo ai
cosiddetti \textbf{teoremi di caratterizzazione}. Questi teoremi ci indicano una qualche  proprietà che
\emph{caratterizza} la norma che stiamo considerando.

\begin{theorem}
	Sia $A \in \R^{n \times n}$, si ha che
	\[ \| A \|_\infty = \max \| A v \|_\infty \]
	sull'insieme dei vettori $v$ tali che $\| v \|_\infty = 1$. Si può dimostrare che questo equivale a
	\[ \max \sum_{j=1}^n |a_{i,j}| \quad i = 1 \dots n \]
	Si va quindi a calcolare la somma dei valori assoluti dei valori che compongono le righe della matrice e
	andando a prendere il massimo di tali somme.
\end{theorem}

\begin{theorem}
	Sia $A \in \R^{n \times n}$, si ha che
	\[ \| A \|_1 = \max \| A v \|_1 \]
	sull'insieme dei vettori $v$ tali che $\| v \|_1 = 1$. Si può dimostrare che questo equivale a
	\[ \max \sum_{i=1}^n |a_{i,j}| \quad j = 1 \dots n \]
	Si va quindi a calcolare la somma dei valori assoluti dei valori che compongono le colonne della matrice e
	andando a prendere il massimo di tali somme.
\end{theorem}

Andiamo ora a vedere perché la norma euclidea non sarà usata per il calcolo della norma matriciale.

\begin{theorem}
	Sia $A \in \R^{n \times n}$, si ha che
	\[ \| A \|_2 = \max \| A v \|_2 \]
	sull'insieme dei vettori $v$ tali che $\| v \|_2 = 1$. Si può dimostrare che questo equivale a
	\[ \sqrt{\rho (A^T A)} \]
	ossia alla radice quadrata del \emph{raggio spettrale} di $A^T A$. Il raggio spettrale di una matrice è il
	modulo dell'autovalore di modulo massimo.
\end{theorem}

Per quanto riguarda i primi due metodi abbiamo un complessità computazionale di $\theta (n^2)$ in quanto dobbiamo
fare la somma di tutti i valori di ogni riga/colonna e poi trovarne il massimo.

Nel caso della norma euclidea invece dobbiamo fare un prodotto tra matrici di costo $\theta (n^3)$ e in più
dobbiamo trovare gli autovalori della matrice. Quest'ultimo problema in particolare diventa molto complesso anche
per matrici di dimensioni contenute. Ecco perché, in generale, non utilizzeremo la norma euclidea per il calcolo
della norma di una matrice. L'unico caso in cui la complessità del problema ci rende possibile utilizzarla è
quello in cui abbiamo $A^T = A$ (matrice simmetrica).
