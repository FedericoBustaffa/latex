\section{Aritmetica di macchina}
Adesso rimane da capire come la macchina mappa i numeri reali in numeri di macchina. Il problema che ci
poniamo è il seguente: sia $x \in \R$, quale numero di macchina $\tx \in \F$ viene associato a $x$
per la rappresentazione?

Associare un numero di macchina $\tx$ a $x$ significa \textbf{approssimare} $x$ commettendo un
\textbf{errore relativo} di rappresentazione
\[ \epsilon_x = \frac{\tx - x}{x} = \frac{\eta_x}{x}, \quad x \neq 0 \]
quanto più piccolo possibile in valore assoluto. La quantità
\[ \eta_x = \tx - x \]
è detta \textbf{errore assoluto} di rappresentazione. Dato $x \in \R$ con $x \neq 0$, distinguiamo 2 casi:
\begin{itemize}
	\item $|x| < \omega$ (\emph{underflow}) o $|x| > \Omega$ (\emph{overflow})
	\item $\omega \leq |x| \leq \Omega$
\end{itemize}
Nel secondo caso le tecniche di approssimazione previste dallo standard IEEE 754 sono:
\begin{itemize}
	\item \textbf{Arrotondamento}: il numero $x$ viene approssimato con il numero di macchina $\tx$
	      più vicino.
	\item \textbf{Troncamento}: il numero $x$ viene approssimato con il più grande numero di macchina
	      $\tx$ il cui valore assoluto risulti minore o uguale al valore assoluto di $x$.
	\item \textbf{Round toward plus infinity}: il numero $x$ viene approssimato al più piccolo numero
	      rappresentabile maggiore del dato.
	\item \textbf{Round toward minus infinity}: il numero $x$ viene approssimato al più grande numero
	      rappresentabile minore del dato.
\end{itemize}
Consideriamo una macchina che opera con troncamento sull'insieme $\F$. Per convenzione indichiamo con
$\trn(x) = \tx$ il risultato dell'approssimazione di $x$ con troncamento e più generalmente $\fl(x)$
l'approssimazione in macchina del dato $x$ nel sistema \emph{floating point} considerato. Il primo risultato
fornisce una maggiorazione uniforme dell'errore di rappresentazione.

\begin{theorem}\label{th: precisione}
	Sia $x \in \R$ con $\omega \leq |x| \leq \Omega$ e sia $\trn(x)$ l'approssimazione di $x$ con troncamento.
	Vale che
	\[ |\epsilon_x| = \left| \frac{\trn(x) - x}{x} \right| \leq B^{1-t} = u \]
	dove $u$ è detta \textbf{precisione di macchina}, la quale non dipende da $x$ e fornisce errori più grandi
	per numeri grandi ed errori piccoli per numeri piccoli.
	\begin{proof}
		Sia $x = (-1)^s \cdot B^p \cdot \alpha$. L'errore assoluto $|\trn(x) - x|$ risulta maggiorato dalla
		distanza tra i due numeri di macchina consecutivi e quindi
		\[ |\trn(x) - x| \leq B^{p-t} \]
		Inoltre $|x| \geq B^{p-1}$. Pertanto vale
		\[ |\epsilon_x| = \left| \frac{\trn(x) - x}{x} \right| \leq \frac{B^{p-t}}{B^{p-1}} = B^{1-t} = u \]
	\end{proof}
\end{theorem}

Il termine \emph{precisione di macchina} potrebbe confondere in quanto si potrebbe pensare che un alto valore
di $u$ significhi una migliore rappresentazione del numero $x$. In realtà la precisione di macchina ci fornisce
un limite superiore per l'errore di rappresentazione su $x$. L'errore di rappresentazione può infatti avere un
valore compreso tra 0 e $u$.

\begin{observation}
	Possiamo osservare che
	\begin{itemize}
		\item La \emph{precisione di macchina} è indipendente dalla grandezza del numero ed è caratteristica
		      dell'aritmetica \emph{floating point} implementata sulla macchina su cui stiamo operando.

		      Se ad esempio operiamo con arrotondamento, la distanza tra $x$ e la sua approssimazione in
		      macchina, e quindi la precisione di macchina, si dimezzano.
		      \[ |\epsilon_x| = \left| \frac{\arr(x) - x}{x} \right| \leq \frac{B^{1 - t}}{2} = u \]
		      Questo perché non abbiamo più una
		      maggiorazione sull'intera ampiezza dell'intervallo tra $\tx$ e il successivo numero di
		      macchina ma sulla metà di tale ampiezza.
		\item Per valutare la precisione di macchina possiamo determinare il più piccolo numero di macchina
		      maggiore di 1. Sia infatti $x$ il numero che cerchiamo, abbiamo che
		      \[ x - 1 = |x - 1| = B^{1-t} \]
		      essendo
		      \[ 1 = B^1 \cdot B^{-1} \]
		      rappresentato con esponente $p = 1$.
		\item Dal teorema \ref{th: precisione} e dall'osservazione appena fatta si ricava che, dato $x \in \R$,
		      in assenza di situazione di \emph{underflow} o \emph{overflow}, per la sua rappresentazione in
		      macchina vale che
		      \[ \fl(x) = x (1 + \epsilon_x), \quad |\epsilon_x| \leq u \]
		      Questa relazione esprime il modo in cui viene generalmente descritto il legame tra un numero reale
		      e la sua rappresentazione in macchina.
	\end{itemize}
\end{observation}

\subsection{Operazioni aritmetiche}
Per le operazioni aritmetiche eseguite in un sistema \emph{floating point} si pone un analogo problema di
approssimazione in quanto il risultato dell'operazione tra due numeri di macchina, in generale, non sarà
un numero di macchina.

Introduciamo dunque una variante delle quattro operazioni fondamentali che indichiamo con $\oplus$, $\ominus$,
$\otimes$ e $\oslash$. Per tali operazioni è ovviamente richiesto che siano interne all'insieme dei numeri
di macchina e che producano un'approssimazione quanto più accurata del risultato esatto.

Prendiamo ad esempio l'operazione di addizione in macchina. Un possibile implementazione è la seguente
\[ \tx \oplus \tilde{y} = \trn(\tx + \tilde{y}) \]
Qui usiamo il troncamento per semplicità ma in realtà l'arrotondamento è il metodo più usato in quanto fornisce
maggiore accuratezza per l'approssimazione. L'operazione scritta sopra produrrà quindi un errore di
approssimazione, il quale viene chiamato \textbf{errore di rappresentazione locale} dell'operazione.

\begin{example}
	Supponiamo di voler calcolare la quantità $f(x) = \frac{x - 1}{x}$. \`E immediato accorgersi che
	\[ \frac{x - 1}{x} = x - \frac{1}{x} \]
	E dunque già si identificano due possibile strade per il calcolo della quantità ma concentriamoci solo
	sulla prima per il momento.

	La prima cosa che fa la macchina è mappare $x$ in $\tx$. A questo dobbiamo stare molto attenti in
	quanto l'algoritmo girerà con
	\[ \tx = x (1 + \epsilon_x), \quad |\epsilon_x| \leq u \]
	e non con $x$, fornendo un risultato potenzialmente del tutto sbagliato.

	La cifra 1 la possiamo assumere come numero di macchina in quanto ogni sistema ragionevole include 1 tra
	i numeri di macchina.

	Se siamo molto fortunati calcolaremo esattamente $f(\tx)$ ma la maggior parte delle volte non sarà
	così e ne calcolaremo un'approssimazione
	\[ g(\tx) = \frac{\tx \ominus 1}{\tx} \]
	Dunque calcoliamo una funzione di fatto differente dalla prima. Sviluppiando l'espressione otteniamo
	\begin{align*}
		g(\tx)  = & \frac{\tx \ominus 1}{\tx}                                                            \\
		=         & \frac{x (1 + \epsilon_x) \ominus 1}{x (1 + \epsilon_x)}                              \\
		=         & \frac{[x (1 + \epsilon_x) - 1](1 + \epsilon_1)}{x (1 + \epsilon_x)} (1 + \epsilon_2)
	\end{align*}
	Dove
	\[ |\epsilon_x| \leq u \quad |\epsilon_1| \leq u \quad |\epsilon_2| \leq u \]
	Noi siamo interessati ad un'analisi \emph{qualitativa} e per farla dobbiamo introdurre una semplificazione:
	Svolgiamo un'\textbf{analisi al primo ordine}, semplificando tutti i termini (di errore) di ordine superiore
	al primo, in quanto considerabili come trascurabili.
	\begin{align*}
		       & \frac{[x (1 + \epsilon_x) - 1](1 + \epsilon_1)}{x (1 + \epsilon_x)} (1 + \epsilon_2) \\
		\doteq & \frac{x (1 + \epsilon_x) - 1}{x (1 + \epsilon_x)} (1 + \epsilon_1 + \epsilon_2)
	\end{align*}
	A questo punto dobbiamo portare $1 + \epsilon_x$ che sta al denominatore, a numeratore. Sapendo che, in
	un'analisi al primo ordine, vale che
	\[ \frac{1}{1 + \epsilon_x} \doteq 1 - \epsilon_x \]
	possiamo sviluppare il calcolo in questo modo
	\begin{align*}
		       & \frac{x (1 + \epsilon_x) - 1}{x (1 + \epsilon_x)} (1 + \epsilon_1 + \epsilon_2)        \\
		\doteq & \frac{x (1 + \epsilon_x) - 1}{x} (1 - \epsilon_x + \epsilon_1 + \epsilon_2)            \\
		=      & \frac{x - 1 + x \epsilon_x}{x} (1 - \epsilon_x + \epsilon_1 + \epsilon_2)              \\
		\doteq & \left( \frac{x - 1}{x} + \epsilon_x \right) (1 - \epsilon_x + \epsilon_1 + \epsilon_2) \\
		\doteq & \frac{x - 1}{x} (1 - \epsilon_x + \epsilon_1 + \epsilon_2) + \epsilon_x
	\end{align*}
	La funzione effettivamente calcolata in macchina è dunque
	\[ g(\tx) = \frac{x - 1}{x} (1 - \epsilon_x + \epsilon_1 + \epsilon_2) + \epsilon_x \]
	L'errore totale sul risultato sarà dunque
	\begin{align*}
		       & \frac{g(\tx) - f(x)}{f(x)}                                                                  \\
		\doteq & \frac{\frac{x - 1}{x} + \frac{x-1}{x}(-\epsilon_x + \epsilon_1 + \epsilon_2) + \epsilon_x -
		\frac{x - 1}{x}}{\frac{x-1}{x}}                                                                      \\
		\doteq & -\epsilon_x + \epsilon_1 + \epsilon_2 + \epsilon_x \frac{x}{x - 1}                          \\
		=      & \epsilon_x \left( \frac{1}{x - 1} \right) + \epsilon_1 + \epsilon_2
	\end{align*}
	Questo ci dice che l'errore finale dipende dagli errori di rappresentazione sui dati e dagli errori locali
	delle operazioni.

	In questo specifico caso l'errore di rappresentazione di $x$ potrebbe essere amplificato molto quando $x$
	tende a 1. Se però $x$ è un numero di macchina (errore di rappresentazione nullo) allora $\epsilon_x = 0$
	e l'errore diventa
	\[ \epsilon_1 + \epsilon_2 \leq 2 u \]
	e il risultato è accurato.
\end{example}