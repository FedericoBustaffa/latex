\chapter{Support Vector Machine}
La \textbf{Support Vector Machine} o \textbf{SVM} \`e un modello lineare usato principalmente per problemi di
classificazione. Quello che per\`o vogliamo fare in questo caso \`e creare il modello sulla base della SLT.

Vogliamo quindi creare un modello sulla base della SLT, ottimizzando per esempio il VC-bound piuttosto che l'errore medio
sui dati di training. Nell'SVM non usiamo la SLT per valutarne la bont\`a, bens\`i, la usiamo per costruirlo.

I punti chiave per la costruzione del modello sono 3:
\begin{itemize}
	\item \textbf{Margine}: vogliamo tenere sotto controllo la complessit\`a del modello andando minimizzare il rischio
	      strutturale. Vedremo un classificatore che cerca di minimizzare l'errore di classificazione in fase di training.
	\item \textbf{Kernel}: si vuole fare un uso efficiente della LBE in modo da ottenere un modello flessibile.
	\item \textbf{Praticit\`a}: si vuole evitare l'overfitting e il problema della dimensionalit\`a dell'input.
\end{itemize}

\section{Hard Margin}
Il primo approccio che vedremo sar\`a per il cosiddetto \textbf{classificatore lineare a margine massimo}, per il quale
facciamo alcune assunzioni che in seguito abbandoneremo in modo da trattare problemi pi\`u complessi:
\begin{itemize}
	\item Il modello tratta un \textbf{problema di classificazione binario}.
	\item Il problema \`e \textbf{linearmente separabile}.
	\item \textbf{Non c'\`e rumore} sui dati in input.
\end{itemize}
Quello che succede nella pratica \`e che, se un problema \`e linearmente separabile, allora il modello trova il
\textbf{massimo margine}, ossia, l'iperpiano che divide esempi positivi e negativi ponendosi per\`o alla massima distanza
sia dagli uni che dagli altri.

Gli esempi, positivi o negativi che siano, che si trovano sul margine sono i \textbf{vettori di supporto}. Questi esempi si
troveranno nella zona di iperpiano tale che
\[ w^T x + b = 1 \]
se l'esempio \`e positivo, mentre
\[ w^T x + b = -1 \]
se l'esempio \`e negativo. In SVM si usa $b$ ma equivale al $w_0$ dei modelli lineari.

Pi\`u in generale, un esempio $x_p$ \`e un vettore di supporto se:
\[ | w^T x_p + b | = 1 \]
Ora possiamo descrivere una relazione che ci dice quando un punto \`e ben classificato o meno. Possiamo infatti dire che
esempio di training \`e ben classificato se
\[ (w^T x_i) y_i \geq 1 \]
Il nostro obbiettivo sar\`a quindi trovare l'iperpiano separatore che classfica correttamente tutti gli esempi di training
con il massimo margine possibile tra positivi e negativi.

Una volta trovato avremo che tutti gli esempi negativi apparterranno alla zona dell'iperpiano tale che
\[ w^T x_p + b \leq -1 \]
mentre tutti i positivi si troveranno nella zona di iperpiano identificata da
\[ w^T x_p + b \geq 1 \]
Prima per\`o di definire l'algoritmo possiamo dire che
\begin{itemize}
	\item Il margine \`e definito da
	      \[ \frac{2}{\| w \|} \]
	      quindi trovare il margine massimo equivale a minimizzare $\| w \|$ o meglio, andremo a minimizzare
	      \[\frac{\| w \|^2}{2}\]
	      che, come vedremo, risulta pi\`u comodo da calcolare.
	\item La VC-dimension dell'SVM \`e \textbf{inversamente proporzionale} alla grandezza del margine.
\end{itemize}
Possiamo quindi dire che l'\textbf{iperpiano ottimale} \`e quello che \emph{massimizza il margine}.

Possiamo trattare il problema come un problema di ottimizzazione scrivendolo in \textbf{forma primale} in questo modo:
\begin{itemize}
	\item \textbf{Funzione obbiettivo}: \emph{minimizzare}
	      \[ \frac{\| w \|^2}{2} \]
	\item \textbf{Vincoli}: i $w$ devono essere tali che
	      \[ (w^T x_p + b) y_p \geq 1 \quad \text{per } p = 1, \dots, l \]
\end{itemize}
Dato che esiste il problema in forma primale possiamo scriverlo anche nella sua \textbf{forma duale}, che non andremo a
trattare nello specifico ma sar\`a interessante analizzarne alcuni aspetti per capire meglio la soluzione che vedremo
tra poco.

Il problema in forma duale \`e questo
\begin{itemize}
	\item \textbf{Funzione obbiettivo}: \emph{massimizzare}
	      \[
		      \sum_{i=1}^l \alpha_i - \sum_{i=1}^l \frac{\alpha_i \alpha_j y_i y_j x_i^T x_j}{2}
		      \quad
		      \text{con } j = 1, \dots, l
	      \]
	\item \textbf{Vincoli}:
	      \[
		      \alpha_i \geq 0 \quad \text{con } i = 1, \dots, l
		      \quad \wedge \quad
		      \sum_{i=1}^l \alpha_i y_i = 0
	      \]
	      dove gli $\alpha$ sono \textbf{moltiplicatori di Lagrange}.
\end{itemize}
Altra considerazione che possiamo fare \`e che il costo computazionale cresce in base al numero di dati in input e non
alla dimensionalit\`a dell'input.

Se avessimo gi\`a gli $\alpha$ a disposizione possiamo calcolare i $w$ in questo modo:
\[ w = \sum_{p=1}^l \alpha_p y_p x_p \]
A questo punto possiamo riscrivere la nostra $h$ come segue:
\[ h(x) = sign(w^T x + b) = sign \left( \sum_{p=1}^l \alpha_p y_p x_p^T x + b \right) \]
Possiamo ridurre il calcolo sapendo conoscendo questa relazione
\[ \alpha_p \neq 0 \Rightarrow x_p \text{ \`e un vettore di supporto} \]
Quindi solo gli $\alpha$ relativi ad un vettore di support sono diversi da zero.

Sapendo questo, possiamo eliminare i casi in cui $\alpha_p = 0$ e quindi possiamo riscrivere la funzione in questo modo:
\[ h(x) = sign \left( \sum_{p \in SV} \alpha_p y_p x_p^T x + b \right) \]
dove $SV$ \`e l'insieme degli indici $p$ tali che $x_p$ \`e un vettore di supporto.

Da notare che il prodotto
\[ x_p^T x \]
\`e un prodotto scalare. Fatto da tenere di conto quando andremo a definire cos'\`e il Kernel.

\section{Soft Margin}
Con il \textbf{soft margin} andiamo ad ammettere qualche errore, ossia qualche esempio all'interno del margine
(\emph{noise tolerance}). Questo ci permette di definire un margine pi\`u ampio e di conseguenza far calare la complessit\`a
del modello.

Per trattare questo nuovo tipo di problema introduciamo le cosiddette \textbf{slack variable}, indicate con $\xi$, che ci
permettono di ammettere errori di classificazione e/o dati rumorosi.

Per riuscire ad usarle, dobbiamo ridefinire il problema in questo modo:
\begin{itemize}
	\item \textbf{Funzione obbiettivo}: \emph{minimizzare}
	      \[ \frac{\| w \|^2}{2} + C \sum_{p=1}^l \xi_p \]
	      dove $C > 0$ \`e un iperparametro che controlla il numero di errori ammessi. Un basso valore di $C$ corrisponde
	      ad un alto grado di ammissibilit\`a degli errori andando per\`o a rischio underfitting.

	      Al contrario un alto valore di $C$ andr\`a a consentire pochi errori in fase di training, andando a generare un
	      modello molto simile a quello visto in precedenza (Hard Margin) andando incontro al rischio di overfitting.
	\item \textbf{Vincoli}:
	      \[ (w x_p + b) y_p \geq 1 - \xi_p \quad \wedge \quad \xi_p \geq 0 \quad \forall p \]
	      Una variabile slack positiva indica un margine troppo stretto o un errore di classificazione e questo ci permette
	      di ignorarlo.
\end{itemize}

\section{Kernel}
Per trattare problemi non linearmente separabili introduciamo il concetto di \textbf{kernel}, per riuscire a trattare il
problema dell'uso efficiente della LBE, in modo da ottenere modelli pi\`u flessibili.

Quello che vogliamo fare \`e \textbf{mappare} i punti definiti nel cosiddetto \textbf{spazio di input}, che, per variabili a
$n$ dimensioni, sar\`a uno spazio $n$-dimensionale, in uno spazio $m$-dimensionale con $m > n$, chiamato
\textbf{feature space}, in cui sono linearmente separabili.

Per farlo abbiamo bisogno di una funzione di trasformazione lineare
\[ \phi : \mathbb{R}^n \rightarrow \mathbb{R}^m \]
che chiameremo \textbf{kernel}.

Ecco che entra in gioco la LBE. Dobbiamo per\`o tenere di conto che l'uso di LBE molto complesse potrebbe essere
computazionalmente troppo dispendioso e potrebbe portare a overfitting.

Per mititgare il problema trattiamo sempre la LBE ma nel contesto di un modello \textbf{regolarizzato}, cio\`e, dipendente
dal margine.

Questo avviene in maniera automatica grazie alla regolarizzazione che l'algoritmo SVM attua naturalmente. La complessit\`a
del modello \`e quindi mantenuta bassa indipendentemente dalla dimensionalit\`a del feature space.

\subsection{LBE nella SVM}
Nella SVM non \`e necessario calcolare $w$ e i dati in input sono in forma di prodotto scalare tra coppie di punti.

La nuova formula di classificazione diventa quindi:
\[ h(x) = sign \left( \sum_{p \in SV} \alpha_p \cdot y_p \cdot \phi^T(x_p) \cdot \phi(x)  \right) \]
In realt\`a non abbiamo neanche bisogno di calcolare direttamente $\phi$ poich\'e possiamo definire una
\textbf{funzione kernel}, che comprende questo prodotto scalare. La funzione che quindi ci interessa \`e la seguente:
\[ h(x) = sign \left( \sum_{p \in SV} \alpha_p \cdot y_p \cdot K(x_p^T, x) \right) \]

\begin{definition}
	Chiamo \textbf{kernel}, una funzione
	\[ K : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} \]
	tale che, per un certo feature space o Hilbert space $X$ e una funzione
	\[ \phi : \mathbb{R}^n \rightarrow X \]
	allora vale
	\[ K(x_i, x_j) = \phi^T(x_i) \cdot \phi(x_j) \]
\end{definition}
Il kernel, potenzialmente, \`e una \textbf{scorciatoia} per calcolare efficientemente il prodotto scalare anche in spazi con
una dimensionalit\`a molto elevata.

Nella pratica noi usiamo il kernel per calcolare il prodotto scalare direttamente nel feature space.

\subsubsection{Kernel comuni}
Vediamo alcuni dei kernel pi\`u popolari nel Machine Learning:
\begin{itemize}
	\item \textbf{Lineare}: quello pi\`u semplice, nel quale $\phi(x)$ equivale a $x$ stesso.
	      \[ K(x_i, x_j) = x_i^T x_j \]
	\item \textbf{Polinomiale}: dove $\phi(x)$ \`e elevato alla $k$
	      \[ K(x_i, x_j) = (1 + x_i^T x_j)^k \]
	\item \textbf{RBF}: si tratta di una funzione Gaussiana in cui $\phi(x)$ ammette input $\infty$-dimensionali.
	      \[ K(x_i, x_j) = e^{-\displaystyle\frac{\| x_i - x_j \|^2}{2 \sigma^2}} \]
	      dove $\sigma$ \`e un iperparametro. Pu\`o essere molto potente ma prona all'overfitting.
\end{itemize}

\subsection{Considerazioni}
Qualche considerazione sull'uso della LBE per SVM:
\begin{itemize}
	\item Per costruire una buona SVM dobbiamo trovare il giusto compromesso tra l'iperparametro $C$ e il kernel (con
	      relativi iperparametri).
	\item Si deve risolvere il problema di ottimizzazione per trovare gli $\alpha$.
	\item Il costo computazionale del modello scala	col numero di dati $l$ e non con la dimensionalit\`a $n$ dell'input.
	\item \`E un modello molto modulabile dato che possiamo usare lo stesso risolutore lineare e cambiare solo il kernel
	      a seconda delle esigenze.
\end{itemize}

Il modello finale \`e espresso come segue:
\[ h(x) = sign \left( \sum_{p \in SV} \alpha_p y_p K(x_p, x) \right) \]

\section{Praticit\`a}
Dobbiamo fare molta attenzione quando utilizziamo la SVM.
\begin{itemize}
	\item Si pu\`o incorrere in overfitting se non si selezionano accuratamente gli iperparametri: $C$, kernel,
	      iperparametri del kernel e via dicendo.
	\item Il trattamento implicito di dati con una dimensionalit\`a molto elevata avviene solo nel feature space, non
	      in input space.
	\item In fase di validazione dobbiamo usare le stesse tecniche rigorose che abbiamo gi\`a visto.
\end{itemize}

\section{Conclusioni}

\begin{itemize}
	\item \`E uno strumento molto potente e popolare: \`e spesso efficiente ma non sempre \`e la scelta migliore.
	\item Proviene dalla teoria.
	\item Combina la LBE (tramite il kernel) con il massimo margine per favorire un approccio flessibile e tenere sotto
	      controllo la complessit\`a.
	\item \`E un modello modulare grazie all'uso del kernel.
\end{itemize}