\chapter{Validazione}
Con la \textbf{validazione} vogliamo trattare il problema di capire quando un modello \`e un \textbf{buon} modello e
anche capire la differenza tra usare algoritmi di Machine Learning e usare \textbf{bene} algoritmi di Machine Learning.

Ricordiamo che fare apprendimento, nello spazio delle generalizzazioni, vuol dire trovare una buona funzione in uno
spazio delle funzioni, a partire da dati noti per fare predizioni su dati non ancora analizzati. Per "buona" si intende
una funzione con una basse errore di generalizzazione.

In questo capitolo cerchiamo di rispondere alle domande:
\begin{itemize}
	\item Una funzione che generalizza bene su dati di training approssimer\`a bene anche su dati non ancora visti ?
	\item \`E una funzione valida ?
\end{itemize}

Nell'ambito della validazione gli obbiettivi principali sono due:
\begin{itemize}
	\item \textbf{Selezione del modello}: si cercano di stimare le performance dei modelli per riuscire a scegliere il
	      migliore.
	\item \textbf{Stima del modello}: una volta scelto il modello, si cerca di stimare l'errore su dati non ancora
	      analizzati. Si cerca di calcolare la \textbf{qualit\`a} del modello.
\end{itemize}
La regola d'oro \`e avere data set \textbf{separati} per ognuno di questi due obbiettivi. Ognuno fatto su misura per
misurare ci\`o di cui abbiamo bisogno.

\section{Approcci alla validazione}
Uno degli approcci standard per la validazione prevede di avere tre data set separati: \textbf{training set},
\textbf{validation set} e \textbf{test set}.
\begin{itemize}
	\item Il training set serve ad allenare il modello per fare un buon fitting dei dati.
	\item Il validation set viene usato su pi\`u modelli per valutare le performance di ognuno e scegliere il migliore
	      (selezione del modello).
	\item Il training set e il validation set a volte sono chiamati con un nome unico: \textbf{set di sviluppo}. Insieme
	      servono a costruire ed individuare il modello finale che andremo ad usare.
	\item Il test set stima l'errore di generalizzazione (per il modello scelto) per dati non ancora analizzati.
\end{itemize}
In genere il training set \`e il data set pi\`u corposo, dato che per fare un modello che fa un buon fitting, in generale,
\`e bene avere tanti esempi.

La dimensione degli altri due data set \`e invece pi\`u contenuta poich\'e servono solo per valutare i nostri modelli.

\subsection{Errori comuni}
Quando siamo in fase di validazione o test dobbiamo fare attenzione a quale data set stiamo usando e a quale proposito. Tra
gli errori pi\`u comuni abbiamo:
\begin{itemize}
	\item Usare il validation set per stimare il grado di bont\`a del modello: il validation set dovrebbe essere fatto
	      su misura per essere applicato a pi\`u modelli e per poter scegliere il migliore.
	\item Usare il test set per scegliere il modello migliore: il test set dovrebbe solo misurare il grado di bont\`a del
	      singolo modello e stimare il suo errore su dati non ancora analizzati.
	\item Fare un modello sulla base del test set \`e un errore: non posso creare un modello su misura per un test set.
	\item Scartare un modello sulla base di un test andato male e farne uno nuovo per poi testarlo ancora sul solito test
	      set. Se il test va male e aggiusto il modello che ho o ne creo uno nuovo devo poi testarlo con un nuovo test set.
\end{itemize}
In generale, soprattutto quando raccogliere dati \`e difficile, la fase di training e validazione \`e fondamentale ed \`e
sempre meglio arrivare a testare il modello quando si ha un buon grado di sicurezza sull'accuratezza di quest'ultimo.

\section{Algoritmo per la validazione}
Definiamo ora un strategia per attuare una buona validazione:
\begin{enumerate}
	\item Separare training, validation e test set.
	\item Cerchiamo la miglior $h_{w, \lambda}$ dove $\lambda$ \`e un iperparametro che pu\`o essere di qualsiasi tipo.
	\item Per ogni $\lambda$ andiamo a cercare la \textbf{migliore} $h_{w, \lambda}$ che minimizza l'errore sul training set.
	      Andiamo quindi a trovare la migliore ipotesi sulla base dell'iperparametro $\lambda$.
	\item Selezioniamo l'ipotesi $h_{w, \lambda}$ che ha il minimo errore sul validation set.
	\item Possiamo anche fare un miglior fitting unendo training set e validation set una volta selezionato il modello
	      (opzionale).
	\item Stimiamo la bont\`a del modello con il test set.
\end{enumerate}

\subsection{Grid search degli iperparametri}
Per trovare il miglior iperparametro $\lambda$ si fa una ricerca a griglia in cui semplicemente manteniamo una tabella
in cui andiamo a mettere vari $\lambda$.

In seguito andiamo a costruire un modello e ne facciamo versioni pi\`u o meno complesse. A questo punto non ci rimane che
usare il validation set per andare a valutare i vari modelli, con relativi gradi di complessit\`a e salvare i risultati
nella tabella.

Il modello scelto sar\`a quello con il minimo errore di generalizzazione.

Per fare una scelta sensata tra i modelli dobbiamo allenarli tutti col solito training set e testarli tutti con il solito
validation set.

Altra regola importante quando si vanno a costruire i vari data set, \`e quella di non fare assunzioni sul comportamento
del modello sul test set, basandoci sul risultato ottenuto dal validation set.

\section{K-Fold Cross-Validation}
Per gestire i casi in cui abbiamo pochi dati disponibili per svolgere le varie operazioni necessarie alla costruzione di
un buon modello, si fa ricorso ad una tecnica chiamata \textbf{K-Fold Cross-Validation}. Questa tecnica si pu\`o applicare
sia in fase di selezione del modello che in fase di stima del modello. \`E composta dai seguenti passi:
\begin{enumerate}
	\item Dividiamo il data set $D$ in $k$ sottoinsiemi mutualmente esclusivi.
	\item Disponiamo i $k$ sottoinsiemi su una riga ripetendo il processo per $k$ righe.
	      \[
		      \begin{bmatrix}
			      D_1   & D_2   & \dots & D_k   \\
			      D_1   & D_2   & \dots & D_k   \\
			      \dots & \dots & \dots & \dots \\
			      D_1   & D_2   & \dots & D_k
		      \end{bmatrix}
	      \]
	      Il risultato, come possiamo vedere, \`e una tabella con $k$ righe tutte uguali composte da $k$ sottoinsiemi
	      di $D$.
	\item Alleniamo la riga $i$ con tutti i data set diversi da $D_i$ e usiamo $D_i$ come validation set.
	\item Ripetiamo il procedimento per tutte le righe.
	\item Facciamo la media delle stime ottenute dai vari validation set (quelli sulla diagonale).
\end{enumerate}
Il risultato finale \`e che abbiamo fatto sia training che validation sul 100\% dei dati senza violare la regola che ci
imponeva di dividere training e validation set. Questo perch\'e, per ogni riga, rifaccio training e validation da zero.

Possiamo applicare lo stesso processo per fare test o possiamo anche combinare training, validation e test set.
